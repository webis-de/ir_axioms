{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parallel Computation of Preference Matrices\n",
    "\n",
    "The notebook below exemplifies how `ir_axioms` can be used to compute preference matrices for all runs submitted to the TREC 2020 Deep Learning passage ranking track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if \"google.colab\" in modules:\n",
    "    !pip install -q ir_axioms[experiments] python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We initialize PyTerrier and import all required libraries and load the data from [ir_datasets](https://ir-datasets.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier import started, init\n",
    "\n",
    "if not started():\n",
    "    init(tqdm=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets and Index\n",
    "\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage/trec-dl-2020/judged\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submitted Runs\n",
    "\n",
    "Define the path where you have stored the submitted run files.\n",
    "(You have to manually download the run files from TREC and store them in a directory of your choice. Then adjust the path below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_dir = Path(\n",
    "    \"/mnt/ceph/storage/data-in-progress/data-research\"\n",
    "    \"/web-search/web-search-trec/trec-system-runs\"\n",
    "    \"/trec29/deep.passages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now load the runs from the run file directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pyterrier.io import read_results\n",
    "\n",
    "run_files = list(result_dir.iterdir())\n",
    "run_results = [\n",
    "    read_results(result_file)\n",
    "    for result_file in tqdm(\n",
    "        run_files, desc=\"Load runs\", unit=\"run\", total=len(run_files)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Concat the retrieved results from all runs in a single data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "\n",
    "all_results = concat(run_results)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Axioms\n",
    "\n",
    "Here we're listing which axioms we want to compute preferences for.\n",
    "Because some axioms require API calls or are computationally expensive, we cache all axioms using `ir_axiom`'s tilde operator (`~`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC,\n",
    "    QTArg,\n",
    "    QTPArg,\n",
    "    aSL,\n",
    "    PROX1,\n",
    "    PROX2,\n",
    "    PROX3,\n",
    "    PROX4,\n",
    "    PROX5,\n",
    "    TFC1,\n",
    "    TFC3,\n",
    "    RS_TF,\n",
    "    RS_TF_IDF,\n",
    "    RS_BM25,\n",
    "    RS_PL2,\n",
    "    RS_QL,\n",
    "    AND,\n",
    "    LEN_AND,\n",
    "    M_AND,\n",
    "    LEN_M_AND,\n",
    "    DIV,\n",
    "    LEN_DIV,\n",
    "    M_TDC,\n",
    "    LEN_M_TDC,\n",
    "    STMC1,\n",
    "    STMC1_f,\n",
    "    STMC2,\n",
    "    STMC2_f,\n",
    "    LNC1,\n",
    "    TF_LNC,\n",
    "    LB1,\n",
    "    REG,\n",
    "    ANTI_REG,\n",
    "    ASPECT_REG,\n",
    "    REG_f,\n",
    "    ANTI_REG_f,\n",
    "    ASPECT_REG_f,\n",
    ")\n",
    "\n",
    "axioms = [\n",
    "    ArgUC(),\n",
    "    QTArg(),\n",
    "    QTPArg(),\n",
    "    aSL(),\n",
    "    LNC1(),\n",
    "    TF_LNC(),\n",
    "    LB1(),\n",
    "    PROX1(),\n",
    "    PROX2(),\n",
    "    PROX3(),\n",
    "    PROX4(),\n",
    "    PROX5(),\n",
    "    REG(),\n",
    "    REG_f(),\n",
    "    ANTI_REG(),\n",
    "    ANTI_REG_f(),\n",
    "    ASPECT_REG(),\n",
    "    ASPECT_REG_f(),\n",
    "    AND(),\n",
    "    LEN_AND(),\n",
    "    M_AND(),\n",
    "    LEN_M_AND(),\n",
    "    DIV(),\n",
    "    LEN_DIV(),\n",
    "    RS_TF(),\n",
    "    RS_TF_IDF(),\n",
    "    RS_BM25(),\n",
    "    RS_PL2(),\n",
    "    RS_QL(),\n",
    "    TFC1(),\n",
    "    TFC3(),\n",
    "    M_TDC(),\n",
    "    LEN_M_TDC(),\n",
    "    STMC1(),\n",
    "    STMC1_f(),\n",
    "    STMC2(),\n",
    "    STMC2_f(),\n",
    "]\n",
    "axioms_cached = [~axiom for axiom in axioms]\n",
    "axiom_names = [axiom.name for axiom in axioms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preference Computation\n",
    "\n",
    "After having defined the axioms to compute, we create a new PyTerrier pipeline that computes preference matrices for the top-10 results of each system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier import Transformer\n",
    "from ir_axioms.backend.pyterrier.transformers import AxiomaticPreferences\n",
    "\n",
    "compute_preferences = Transformer.from_df(all_results) % 10 >> AxiomaticPreferences(\n",
    "    axioms=axioms,\n",
    "    # axioms=axioms_cached,\n",
    "    axiom_names=axiom_names,\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To speed up computation, let's distribute preference matrix computation across 4 cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compute_preferences = compute_preferences.parallel(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next step, we parallely compute the preference matrices (line 5)\n",
    "and measure the elapsed time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "\n",
    "time = perf_counter_ns()\n",
    "\n",
    "preferences = compute_preferences.transform(dataset.get_topics())\n",
    "\n",
    "elapsed_time = perf_counter_ns() - time\n",
    "elapsed_time_seconds = elapsed_time / 1_000_000_000\n",
    "print(f\"Elapsed time: {elapsed_time_seconds:.2f}s\")\n",
    "preferences_per_second = len(preferences) / elapsed_time_seconds\n",
    "print(f\"Preferences per second: {preferences_per_second:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here's the resulting Pandas `DataFrame` containing all preferences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Note About Backends\n",
    "\n",
    "The parallelization is implemented in [PyTerrier](https://pyterrier.readthedocs.io/en/latest/parallel.html).\n",
    "In this example we use the default [Joblib](https://joblib.readthedocs.io/en/latest/) backend which splits computation per query and runs on multiple cores on the same machine.\n",
    "\n",
    "However, you could also use the [Ray](https://www.ray.io/) backend.\n",
    "With Ray you can connect to remote clusters and distribute the workload across multiple machines\n",
    "(e.g. [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes.html), [Hadoop/Spark](https://docs.ray.io/en/latest/cluster/yarn.html), or [Slurm](https://docs.ray.io/en/latest/cluster/slurm.html)).\n",
    "Please refer to the [Ray documentation](https://docs.ray.io/en/latest/) for detailed instructions on how to connect your cluster.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1a2212cc5ec32f4daeb9f9834a8cd8a7db46f8766913eafbf4fe810a9194b3a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

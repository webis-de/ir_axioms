{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Re-Ranking Comparison\n",
    "\n",
    "The notebook below showcases different axiomatic re-ranking approaches in `ir_axioms` using PyTerrier:\n",
    "\n",
    "1. KwikSort with manually combined axioms,\n",
    "2. KwikSort with an estimation of the ORACLE axiom, and\n",
    "3. aggregated axiom preferences as features for LamdaMART.\n",
    "\n",
    "We use run files and qrels from the passage retrieval task of the TREC Deep Learning track in 2019 and 2020 as example (using BM25 as a baseline).\n",
    "In this notebook, we evaluate nDCG@10, reciprocal rank, and average precision for the baseline and the re-ranked pipelines using PyTerrier's standard `Experiment` functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if \"google.colab\" in modules:\n",
    "    !pip install -q ir_axioms[experiments] python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets and Index\n",
    "\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")\n",
    "dataset_train: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2019/judged\")\n",
    "dataset_test: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2020/judged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]\n",
    "results_dir = Path(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Run\n",
    "\n",
    "We use PyTerrier's `BatchRetrieve` to create a baseline search pipeline for retrieving with BM25 from the index we just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.batchretrieve import BatchRetrieve\n",
    "\n",
    "bm25 = BatchRetrieve(str(index_dir.absolute()), wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Combine and Import Axioms\n",
    "\n",
    "Here we're listing which axioms we want to use in our experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC,\n",
    "    QTArg,\n",
    "    QTPArg,\n",
    "    aSL,\n",
    "    PROX1,\n",
    "    PROX2,\n",
    "    PROX3,\n",
    "    PROX4,\n",
    "    PROX5,\n",
    "    TFC1,\n",
    "    TFC3,\n",
    "    RS_TF,\n",
    "    RS_TF_IDF,\n",
    "    RS_BM25,\n",
    "    RS_PL2,\n",
    "    RS_QL,\n",
    "    AND,\n",
    "    LEN_AND,\n",
    "    M_AND,\n",
    "    LEN_M_AND,\n",
    "    DIV,\n",
    "    LEN_DIV,\n",
    "    M_TDC,\n",
    "    LEN_M_TDC,\n",
    "    STMC1,\n",
    "    STMC1_f,\n",
    "    STMC2,\n",
    "    STMC2_f,\n",
    "    LNC1,\n",
    "    TF_LNC,\n",
    "    LB1,\n",
    "    REG,\n",
    "    ANTI_REG,\n",
    "    REG_f,\n",
    "    ANTI_REG_f,\n",
    "    ASPECT_REG,\n",
    "    ASPECT_REG_f,\n",
    "    ORIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we combine many of the axioms implemented in `ir_axioms` to form a majority voting.\n",
    "That is, we only want to keep preferences, where at least 50% (or 0.5) of the axioms agree.\n",
    "Because some axioms require API calls or are computationally expensive, we cache the voting result using the tilde operator (`~`).\n",
    "We are going to use that vote axiom in a `KwikSortReranker` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import VoteAxiom\n",
    "\n",
    "majority_vote_axiom = (\n",
    "    ~VoteAxiom(\n",
    "        [\n",
    "            ArgUC(),\n",
    "            QTArg(),\n",
    "            QTPArg(),\n",
    "            aSL(),\n",
    "            LNC1(),\n",
    "            TF_LNC(),\n",
    "            LB1(),\n",
    "            PROX1(),\n",
    "            PROX2(),\n",
    "            PROX3(),\n",
    "            PROX4(),\n",
    "            PROX5(),\n",
    "            REG(),\n",
    "            REG_f(),\n",
    "            ANTI_REG(),\n",
    "            ANTI_REG_f(),\n",
    "            ASPECT_REG(),\n",
    "            ASPECT_REG_f(),\n",
    "            AND(),\n",
    "            LEN_AND(),\n",
    "            M_AND(),\n",
    "            LEN_M_AND(),\n",
    "            DIV(),\n",
    "            LEN_DIV(),\n",
    "            RS_TF(),\n",
    "            RS_TF_IDF(),\n",
    "            RS_BM25(),\n",
    "            RS_PL2(),\n",
    "            RS_QL(),\n",
    "            TFC1(),\n",
    "            TFC3(),\n",
    "            M_TDC(),\n",
    "            LEN_M_TDC(),\n",
    "            STMC1(),\n",
    "            STMC1_f(),\n",
    "            STMC2(),\n",
    "            STMC2_f(),\n",
    "        ],\n",
    "        minimum_votes=0.5,\n",
    "    )\n",
    "    | ORIG()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, for estimating the ORACLE axiom and for generating axiomatic features for learning to rank with LambdaMART, we define a list of all axioms that we want to use in our experiments.\n",
    "Again, we implement caching for the axioms (using `~`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_axioms = [\n",
    "    ~ArgUC(),\n",
    "    ~QTArg(),\n",
    "    ~QTPArg(),\n",
    "    ~aSL(),\n",
    "    ~LNC1(),\n",
    "    ~TF_LNC(),\n",
    "    ~LB1(),\n",
    "    ~PROX1(),\n",
    "    ~PROX2(),\n",
    "    ~PROX3(),\n",
    "    ~PROX4(),\n",
    "    ~PROX5(),\n",
    "    ~REG(),\n",
    "    ~REG_f(),\n",
    "    ~ANTI_REG(),\n",
    "    ~ANTI_REG_f(),\n",
    "    ~ASPECT_REG(),\n",
    "    ~ASPECT_REG_f(),\n",
    "    ~AND(),\n",
    "    ~LEN_AND(),\n",
    "    ~M_AND(),\n",
    "    ~LEN_M_AND(),\n",
    "    ~DIV(),\n",
    "    ~LEN_DIV(),\n",
    "    ~RS_TF(),\n",
    "    ~RS_TF_IDF(),\n",
    "    ~RS_BM25(),\n",
    "    ~RS_PL2(),\n",
    "    ~RS_QL(),\n",
    "    ~TFC1(),\n",
    "    ~TFC3(),\n",
    "    ~M_TDC(),\n",
    "    ~LEN_M_TDC(),\n",
    "    ~STMC1(),\n",
    "    ~STMC1_f(),\n",
    "    ~STMC2(),\n",
    "    ~STMC2_f(),\n",
    "    ORIG(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Re-ranking Approaches\n",
    "\n",
    "We will now compare the three different axiomatic re-ranking approaches.\n",
    "Please refer to the other notebooks in this repository for more detailed explanations of each of the approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KwikSort Re-ranking\n",
    "\n",
    "For the first re-ranker, we re-rank the top-20 baseline results using the KwikSort algorithm, using our previously defined vote axiom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.modules.pivot import MiddlePivotSelection\n",
    "from ir_axioms.backend.pyterrier.transformers import KwikSortReranker\n",
    "\n",
    "kwiksort = bm25 % 20 >> KwikSortReranker(\n",
    "    axiom=majority_vote_axiom,\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    pivot_selection=MiddlePivotSelection(),\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KwikSort Re-ranking with Estimating the ORACLE Axiom\n",
    "\n",
    "The second re-ranker works by estimating the ORACLE axiom using preferences from all reference axioms using a random forest classifier.\n",
    "The resulting output preferences are used with KwikSort to re-rank the top-20 baseline results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ir_axioms.modules.pivot import MiddlePivotSelection\n",
    "from ir_axioms.backend.pyterrier.estimator import EstimatorKwikSortReranker\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    max_depth=3,\n",
    ")\n",
    "kwiksort_random_forest = bm25 % 20 >> EstimatorKwikSortReranker(\n",
    "    axioms=all_axioms,\n",
    "    estimator=random_forest,\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    pivot_selection=MiddlePivotSelection(),\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We fit the estimator using preferences from the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kwiksort_random_forest.fit(dataset_train.get_topics(), dataset_train.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregating Axiomatic Features for LTR with LambdaMART\n",
    "\n",
    "For the third re-ranker, we aggregate axiomatic preferences into three features per axiom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.backend.pyterrier.transformers import AggregatedAxiomaticPreferences\n",
    "\n",
    "aggregations = [\n",
    "    lambda prefs: sum(p >= 0 for p in prefs) / len(prefs),\n",
    "    lambda prefs: sum(p == 0 for p in prefs) / len(prefs),\n",
    "    lambda prefs: sum(p <= 0 for p in prefs) / len(prefs),\n",
    "]\n",
    "features = ~(\n",
    "    bm25 % 20\n",
    "    >> AggregatedAxiomaticPreferences(\n",
    "        axioms=all_axioms,\n",
    "        index=index_dir,\n",
    "        aggregations=aggregations,\n",
    "        dataset=dataset_name,\n",
    "        cache_dir=cache_dir,\n",
    "        verbose=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After aggregating the preference features, we initialize a LambdaMART ranker for optimizing nDCG@10 and apply it to the top-20 baseline results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "from pyterrier.ltr import apply_learned_model\n",
    "\n",
    "lambda_mart = LGBMRanker(\n",
    "    num_iterations=1000,\n",
    "    metric=\"ndcg\",\n",
    "    eval_at=[10],\n",
    "    importance_type=\"gain\",\n",
    ")\n",
    "ltr = features >> apply_learned_model(lambda_mart, form=\"ltr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also fit the re-ranker using preferences from the training dataset (using the last 5 topics as the validation dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ltr.fit(\n",
    "    dataset_train.get_topics()[:-5],\n",
    "    dataset_train.get_qrels(),\n",
    "    dataset_train.get_topics()[-5:],\n",
    "    dataset_train.get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimental Evaluation\n",
    "\n",
    "Because our axiomatic re-rankers are PyTerrier modules, we can now use PyTerrier's `Experiment` interface to evaluate various metrics and to compare our new approaches to the BM25 baseline ranking.\n",
    "Refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io/en/latest/experiments.html) to learn more about running experiments.\n",
    "(We concatenate results from the Baseline ranking for the ranking positions after the top-20 using the `^` operator.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.pipelines import Experiment\n",
    "from ir_measures import nDCG, MAP, RR\n",
    "\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "experiment = Experiment(\n",
    "    [bm25, kwiksort ^ bm25, kwiksort_random_forest ^ bm25, ltr ^ bm25],\n",
    "    dataset_test.get_topics(),\n",
    "    dataset_test.get_qrels(),\n",
    "    [nDCG @ 10, RR, MAP],\n",
    "    [\"BM25\", \"KwikSort\", \"KwikSort Random Forest\", \"Axiomatic LTR\"],\n",
    "    verbose=True,\n",
    ")\n",
    "experiment.sort_values(by=\"nDCG@10\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

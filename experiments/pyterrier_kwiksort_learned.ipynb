{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# KwikSort Re-Ranking with Estimating the ORACLE Axiom\n",
    "\n",
    "The notebook below exemplifies how `ir_axioms` can be used to re-rank a result set in PyTerrier using the KwikSort algorithm and an estimation of the ORACLE axiom.\n",
    "We use run files and qrels from the passage retrieval task of the TREC Deep Learning track in 2019 and 2020 as example (using BM25 as a baseline).\n",
    "In this notebook, we first train the ORACLE axiom estimation using preferences inferred from 2019 qrels and topics.\n",
    "Then, we re-rank using that trained `EstimatorAxiom` and evaluate nDCG@10, reciprocal rank, and average precision for the baseline and the re-ranked pipeline using PyTerrier's standard `Experiment` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if 'google.colab' in modules:\n",
    "    !pip install -q ir_axioms[experiments]>=1.0 python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets and Index\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")\n",
    "dataset_train: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2019/judged\")\n",
    "dataset_test: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2020/judged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"experiments/cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(\n",
    "        dataset.get_corpus_iter(),\n",
    "        fields=[\"text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Run\n",
    "\n",
    "We use PyTerrier's `BatchRetrieve` to create a baseline search pipeline for retrieving with BM25 from the index we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.batchretrieve import BatchRetrieve\n",
    "\n",
    "bm25 = BatchRetrieve(str(index_dir.absolute()), wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Axioms\n",
    "Here we're listing which axioms we want to use in our experiments.\n",
    "Because some axioms require API calls or are computationally expensive, we cache all axioms using `ir_axiom`'s tilde operator (`~`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC, QTArg, QTPArg, aSL, PROX1, PROX2, PROX3, PROX4, PROX5, TFC1, TFC3, \n",
    "    AND, LEN_AND, M_AND, LEN_M_AND, DIV, LEN_DIV, M_TDC, LEN_M_TDC, STMC1, STMC2, LNC1, TF_LNC, LB1,\n",
    "    REG, ANTI_REG, ASPECT_REG, ORIG\n",
    ")\n",
    "from ir_axioms.integrations.pyterrier.utils import inject_pyterrier\n",
    "\n",
    "inject_pyterrier(\n",
    "    index_location=index_dir,\n",
    "    text_field=None,\n",
    "    dataset=dataset_name,\n",
    ")\n",
    "\n",
    "axioms = [\n",
    "    ArgUC().cached(cache_dir / \"ArgUC\"),\n",
    "    QTArg().cached(cache_dir / \"QTArg\"),\n",
    "    QTPArg().cached(cache_dir / \"QTPArg\"),\n",
    "    aSL().cached(cache_dir / \"aSL\"),\n",
    "    LNC1().cached(cache_dir / \"LNC1\"),\n",
    "    TF_LNC().cached(cache_dir / \"TF_LNC\"),\n",
    "    LB1().cached(cache_dir / \"LB1\"),\n",
    "    PROX1().cached(cache_dir / \"PROX1\"),\n",
    "    PROX2().cached(cache_dir / \"PROX2\"),\n",
    "    PROX3().cached(cache_dir / \"PROX3\"),\n",
    "    PROX4().cached(cache_dir / \"PROX4\"),\n",
    "    PROX5().cached(cache_dir / \"PROX5\"),\n",
    "    REG().cached(cache_dir / \"REG\"),\n",
    "    ANTI_REG().cached(cache_dir / \"ANTI_REG\"),\n",
    "    ASPECT_REG().cached(cache_dir / \"ASPECT_REG\"),\n",
    "    AND().cached(cache_dir / \"AND\"),\n",
    "    LEN_AND().cached(cache_dir / \"LEN_AND\"),\n",
    "    M_AND().cached(cache_dir / \"M_AND\"),\n",
    "    LEN_M_AND().cached(cache_dir / \"LEN_M_AND\"),\n",
    "    DIV().cached(cache_dir / \"DIV\"),\n",
    "    LEN_DIV().cached(cache_dir / \"LEN_DIV\"),\n",
    "    TFC1().cached(cache_dir / \"TFC1\"),\n",
    "    TFC3().cached(cache_dir / \"TFC3\"),\n",
    "    M_TDC().cached(cache_dir / \"M_TDC\"),\n",
    "    LEN_M_TDC().cached(cache_dir / \"LEN_M_TDC\"),\n",
    "    STMC1().cached(cache_dir / \"STMC1\"),\n",
    "    STMC2().cached(cache_dir / \"STMC2\"),\n",
    "    ORIG(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## KwikSort Re-ranking with Estimating the ORACLE Axiom\n",
    "We have now defined the axioms with which we want to estimate the ORACLE axiom.\n",
    "To remind, the ORACLE axiom replicates the perfect ordering induced by human relevance judgments (i.e. from qrels).\n",
    "We combine the preferences from all axioms in a random forest classifier.\n",
    "The resulting output preferences can be used with KwikSort to re-rank the top-20 baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ir_axioms.tools import MiddlePivotSelection\n",
    "from ir_axioms.integrations.pyterrier.estimator import EstimatorKwikSortReranker\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    max_depth=3,\n",
    ")\n",
    "kwiksort_random_forest = bm25 % 20 >> EstimatorKwikSortReranker(\n",
    "    axioms=axioms,\n",
    "    estimator=random_forest,\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    text_field=None,\n",
    "    pivot_selection=MiddlePivotSelection(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After setting up the trainable PyTerrier module, we pass in training topics and relevance judgments for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kwiksort_random_forest.fit(dataset_train.get_topics(), dataset_train.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimental Evaluation\n",
    "Because our axiomatic re-rankers are PyTerrier modules, we can now use PyTerrier's `Experiment` interface to evaluate various metrics and to compare our new approach to the BM25 baseline ranking.\n",
    "Refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io/en/latest/experiments.html) to learn more about running experiments.\n",
    "(We concatenate results from the Baseline ranking for the ranking positions after the top-20 using the `^` operator.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.pipelines import Experiment\n",
    "from ir_measures import nDCG, MAP, RR\n",
    "\n",
    "experiment = Experiment(\n",
    "    [bm25, kwiksort_random_forest ^ bm25],\n",
    "    dataset_test.get_topics(),\n",
    "    dataset_test.get_qrels(),\n",
    "    [nDCG @ 10, RR, MAP],\n",
    "    [\"BM25\", \"KwikSort Random Forest\"],\n",
    "    verbose=True,\n",
    ")\n",
    "experiment.sort_values(by=\"nDCG@10\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extra: Feature Importances\n",
    "Inspecting the feature importances from the random forest classifier can help to identify axioms that are not used for re-ranking.\n",
    "If an axiom's feature importance is zero for most of your applications, you may consider omitting it from the ranking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

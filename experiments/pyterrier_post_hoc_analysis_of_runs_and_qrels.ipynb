{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Post-Hoc Analysis of Rankings and Relevance Judgments\n",
    "\n",
    "The notebook below examplifies how `ir_axioms` allows the post-hoc analysis of run files and qrels using the passage retrieval task of the TREC Deep Learning track in 2019 and 2020 as example (using a single BM25 run). In this notebook, we calculate the distribution of axiom preferences in rankings and evaluate the consistency of rankings and relevance judgments with retrieval axioms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if \"google.colab\" in modules:\n",
    "    !pip install -q ir_axioms[experiments] python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We initialize PyTerrier and import all required libraries and load the data from [ir_datasets](https://ir-datasets.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier import started, init\n",
    "\n",
    "if not started():\n",
    "    init(tqdm=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets and Index\n",
    "\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")\n",
    "dataset_train: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2019/judged\")\n",
    "dataset_test: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2020/judged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Run\n",
    "\n",
    "We use PyTerrier's `BatchRetrieve` to create a baseline search pipeline for retrieving with BM25 from the index we just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.batchretrieve import BatchRetrieve\n",
    "\n",
    "bm25 = BatchRetrieve(str(index_dir.absolute()), wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Axioms\n",
    "\n",
    "Here we're listing which axioms we want to use in our experiments.\n",
    "Because some axioms require API calls or are computationally expensive, we cache all axioms using `ir_axiom`'s tilde operator (`~`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC,\n",
    "    QTArg,\n",
    "    QTPArg,\n",
    "    aSL,\n",
    "    PROX1,\n",
    "    PROX2,\n",
    "    PROX3,\n",
    "    PROX4,\n",
    "    PROX5,\n",
    "    TFC1,\n",
    "    TFC3,\n",
    "    RS_TF,\n",
    "    RS_TF_IDF,\n",
    "    RS_BM25,\n",
    "    RS_PL2,\n",
    "    RS_QL,\n",
    "    AND,\n",
    "    LEN_AND,\n",
    "    M_AND,\n",
    "    LEN_M_AND,\n",
    "    DIV,\n",
    "    LEN_DIV,\n",
    "    M_TDC,\n",
    "    LEN_M_TDC,\n",
    "    STMC1,\n",
    "    STMC1_f,\n",
    "    STMC2,\n",
    "    STMC2_f,\n",
    "    LNC1,\n",
    "    TF_LNC,\n",
    "    LB1,\n",
    "    REG,\n",
    "    ANTI_REG,\n",
    "    REG_f,\n",
    "    ANTI_REG_f,\n",
    "    ASPECT_REG,\n",
    "    ASPECT_REG_f,\n",
    ")\n",
    "\n",
    "axioms = [\n",
    "    ~ArgUC(),\n",
    "    ~QTArg(),\n",
    "    ~QTPArg(),\n",
    "    ~aSL(),\n",
    "    ~LNC1(),\n",
    "    ~TF_LNC(),\n",
    "    ~LB1(),\n",
    "    ~PROX1(),\n",
    "    ~PROX2(),\n",
    "    ~PROX3(),\n",
    "    ~PROX4(),\n",
    "    ~PROX5(),\n",
    "    ~REG(),\n",
    "    ~REG_f(),\n",
    "    ~ANTI_REG(),\n",
    "    ~ANTI_REG_f(),\n",
    "    ~ASPECT_REG(),\n",
    "    ~ASPECT_REG_f(),\n",
    "    ~AND(),\n",
    "    ~LEN_AND(),\n",
    "    ~M_AND(),\n",
    "    ~LEN_M_AND(),\n",
    "    ~DIV(),\n",
    "    ~LEN_DIV(),\n",
    "    ~RS_TF(),\n",
    "    ~RS_TF_IDF(),\n",
    "    ~RS_BM25(),\n",
    "    ~RS_PL2(),\n",
    "    ~RS_QL(),\n",
    "    ~TFC1(),\n",
    "    ~TFC3(),\n",
    "    ~M_TDC(),\n",
    "    ~LEN_M_TDC(),\n",
    "    ~STMC1(),\n",
    "    ~STMC1_f(),\n",
    "    ~STMC2(),\n",
    "    ~STMC2_f(),\n",
    "]\n",
    "axiom_names = [axiom.axiom.name for axiom in axioms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Axiomatic Experiments\n",
    "\n",
    "The `AxiomaticExperiment` class provides the entry-point to the post-hoc analysis of rankings and relevance judgments.\n",
    "We pass it the retrieval pipelines to evaluate, dataset and index, and the axioms to compute preferences for.\n",
    "With the `depth` parameter you can control how much preferences are being computed.\n",
    "\n",
    "Computing time roughly scales with `len(retrieval_systems)` x `len(topics)` x `depth` x `depth` x `len(axioms)`.\n",
    "Additional parameters help optimize computational costs by filtering missing qrels and/or topics.\n",
    "In our experience, caching helps most with multiple runs for the same benchmark (like TREC runs), because different run's result sets often overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.backend.pyterrier.experiment import AxiomaticExperiment\n",
    "\n",
    "experiment = AxiomaticExperiment(\n",
    "    retrieval_systems=[bm25],\n",
    "    topics=dataset_test.get_topics(),\n",
    "    qrels=dataset_test.get_qrels(),\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    axioms=axioms,\n",
    "    axiom_names=axiom_names,\n",
    "    depth=10,\n",
    "    filter_by_qrels=False,\n",
    "    filter_by_topics=False,\n",
    "    verbose=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate All Preferences\n",
    "\n",
    "With the `preference` member you can compute a `DataFrame` containing all pairwise preferences up to the specified depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment.preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Inconsistent Document Pairs\n",
    "\n",
    "Investigate document pairs that are inconsistent with retrieval axioms (and relevance judgments).\n",
    "These can provide insights to improve the evaluated ranking function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment.inconsistent_pairs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Consistency of Runs with Axioms\n",
    "\n",
    "This evaluation shows how consistent axioms are with the run's original order (ORIG) and the relevance judgments (ORACLE).\n",
    "High consistency with the ORACLE axiom indicates that for your run, the axiom captures the relevance judgments well.\n",
    "High consistency with the ORIG axiom indicates that your run adheres to the axiom's constraint well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment.preference_consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Preference Distribution\n",
    "\n",
    "Calculating the distribution of each axiom's preferences compared to the original ranking order (ORIG) can help identify decisive axioms for your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment.preference_distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

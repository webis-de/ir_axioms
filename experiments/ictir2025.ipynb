{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICTIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan, isnan\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from numpy import stack, sign\n",
    "from numpy.typing import NDArray\n",
    "from pandas import read_json, DataFrame, concat, Categorical, read_csv\n",
    "from pyterrier.datasets import get_dataset\n",
    "from pyterrier.io import read_results\n",
    "from seaborn import FacetGrid, lineplot, color_palette\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ir_axioms.axiom import (\n",
    "    Axiom,\n",
    "    # Adapted axioms\n",
    "    GEN_TFC1,\n",
    "    GEN_LNC1,\n",
    "    GEN_TF_LNC,\n",
    "    GEN_REG,\n",
    "    GEN_AND,\n",
    "    GEN_DIV,\n",
    "    GEN_STMC1,\n",
    "    GEN_STMC2,\n",
    "    GEN_PROX1,\n",
    "    GEN_PROX2,\n",
    "    GEN_PROX3,\n",
    "    GEN_PROX4,\n",
    "    GEN_PROX5,\n",
    "    GEN_aSL,\n",
    "    # Generation-specific axioms\n",
    "    CLAR1,\n",
    "    CLAR2,\n",
    "    CONS3,\n",
    "    CONS2,\n",
    "    CONS1,\n",
    "    CORR1,\n",
    "    COV1,\n",
    "    COV2,\n",
    "    COV3,\n",
    "    COH1,\n",
    "    COH2,\n",
    "    # Oracle axioms\n",
    "    TrecRagNuggetAxiom,\n",
    "    TrecRagCrowdAxiom,\n",
    "    # Utility axioms\n",
    "    MajorityVoteAxiom,\n",
    "    # Original axioms\n",
    "    TrecRagLlmOrigAxiom,\n",
    ")\n",
    "from ir_axioms.model import GenerationInput, GenerationOutput\n",
    "from ir_axioms.tools import SpacyEntitiesAspectExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path(\"../data/cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_assignments_path = Path(\"../data/nugget_assignment.20241108.jl\")\n",
    "rag_assignments_path = Path(\"../data/nugget_assignment.20241218.jsonl\")\n",
    "rag_crowd_responses_path = Path(\"../data/crowd/responses.jsonl.gz\")\n",
    "rag_crowd_ratings_path = Path(\"../data/crowd/ratings.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rag_assignments_outputs_df() -> DataFrame:\n",
    "    df = read_json(rag_assignments_path, lines=True)\n",
    "    df.drop(columns=[\"response_length\", \"nuggets\"], inplace=True)\n",
    "    df[\"query\"] = df[\"query\"].fillna(\"\")\n",
    "    df[\"answer_text\"] = df[\"answer_text\"].fillna(\"\")\n",
    "    df.rename(columns={\"run_id\": \"name\", \"answer_text\": \"text\"}, inplace=True)\n",
    "    df[\"context\"] = nan\n",
    "    return df[[\"qid\", \"query\", \"context\", \"name\", \"text\"]]\n",
    "\n",
    "# read_rag_assignments_outputs_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rag_crowd_outputs_df() -> DataFrame:\n",
    "    df = read_json(rag_crowd_responses_path, lines=True)\n",
    "\n",
    "    df_ratings = read_json(rag_crowd_ratings_path, lines=True)\n",
    "    df = df[df[\"topic\"].isin(df_ratings[\"query_id\"])]\n",
    "\n",
    "    df[\"name\"] = df[\"kind\"] + \"-\" + df[\"style\"]\n",
    "    # df = df[df[\"kind\"] == \"human\"]\n",
    "    df.rename(\n",
    "        columns={\"topic\": \"qid\", \"references_texts\": \"context\", \"raw_text\": \"text\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df[[\"qid\", \"query\", \"context\", \"name\", \"text\"]]\n",
    "\n",
    "\n",
    "# read_rag_crowd_outputs_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inputs_outputs(df: DataFrame) -> list[tuple[GenerationInput, list[GenerationOutput]]]:\n",
    "    data: list[tuple[GenerationInput, list[GenerationOutput]]] = []\n",
    "    runs = sorted(df[\"name\"].unique())\n",
    "    for (query_id, query), df_query in df.groupby(\n",
    "        [\"qid\", \"query\"], group_keys=False, sort=False, as_index=False,\n",
    "    ):\n",
    "        contexts = {\n",
    "            tuple(row[\"context\"]) \n",
    "            if isinstance(row[\"context\"], Sequence) else None\n",
    "            for _, row in df_query.iterrows()\n",
    "        }\n",
    "        if len(contexts) > 1:\n",
    "            raise ValueError(f\"Multiple contexts for query {query_id}: {'; '.join(contexts)}\")\n",
    "        context = next(iter(contexts))\n",
    "        input = GenerationInput(\n",
    "            id=query_id,\n",
    "            text=query,\n",
    "            context=context,\n",
    "        )\n",
    "        df_query = df_query.drop(columns=[\"qid\", \"query\"])\n",
    "        outputs = {\n",
    "            row[\"name\"]: GenerationOutput(\n",
    "                id=row[\"name\"],\n",
    "                text=row[\"text\"],\n",
    "            )\n",
    "            for _, row in df_query.iterrows()\n",
    "        }\n",
    "        data.append(\n",
    "            (\n",
    "                input,\n",
    "                [\n",
    "                    outputs.get(\n",
    "                        run,\n",
    "                        GenerationOutput(\n",
    "                            id=run,\n",
    "                            text=\"\",\n",
    "                        ),\n",
    "                    )\n",
    "                    for run in runs\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_inputs_outputs(read_rag_assignments_outputs_df())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_inputs_outputs(read_rag_crowd_outputs_df())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_oracle_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput], str]] = [\n",
    "    *[\n",
    "        (\n",
    "            f\"ORACLE-NUGGET-{score_type.upper()}{\"-STRICT\" if strict else ''}{f'-{margin_fraction:.1f}' if margin_fraction > 0.0 else ''}\",\n",
    "            TrecRagNuggetAxiom(\n",
    "                assignments_path=rag_assignments_path,\n",
    "                score_type=score_type,\n",
    "                strict=False,\n",
    "                margin_fraction=margin_fraction,\n",
    "            ),\n",
    "            \"assignments\",\n",
    "        )\n",
    "        for score_type in (\n",
    "            \"all\",\n",
    "            \"vital\",\n",
    "            \"weighted\"\n",
    "        )\n",
    "        for strict in (\n",
    "            True,\n",
    "            False,\n",
    "        )\n",
    "        for margin_fraction in (\n",
    "            0.0,\n",
    "            # 0.1,\n",
    "        )\n",
    "    ],\n",
    "    *[\n",
    "        (\n",
    "            f\"ORACLE-CROWD-{utility_type.upper().replace(' ', '-')}{f'-{margin_fraction:.1f}' if margin_fraction > 0.0 else ''}\",\n",
    "            TrecRagCrowdAxiom(\n",
    "                responses_path=rag_crowd_responses_path,\n",
    "                ratings_path=rag_crowd_ratings_path,\n",
    "                utility_type=utility_type,\n",
    "                margin_fraction=margin_fraction,\n",
    "            ),\n",
    "            \"crowd\",\n",
    "        )\n",
    "        for utility_type in (\n",
    "            \"overall\",\n",
    "            \"logical coherence\",\n",
    "            \"stylistic coherence\",\n",
    "            \"coherence\",\n",
    "            \"internal consistency\",\n",
    "            \"consistency\",\n",
    "            \"topical correctness\",\n",
    "            \"correctness\",\n",
    "            \"broad coverage\",\n",
    "            \"deep coverage\",\n",
    "            \"coverage\",\n",
    "        )\n",
    "        for margin_fraction in (\n",
    "            0.0,\n",
    "            # 0.1,\n",
    "        )\n",
    "    ],\n",
    "]\n",
    "rag_oracle_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"), run_type)\n",
    "    for name, axiom, run_type in rag_oracle_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_oracle_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom, axiom_run_type in tqdm(rag_oracle_axioms, desc=f\"Axioms: {data_run_type}\", unit=\"axiom\")\n",
    "    if data_run_type == axiom_run_type\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted retrieval axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput]]] = [\n",
    "    (\"GEN-TFC1\", GEN_TFC1()),\n",
    "    (\"GEN-LNC1\", GEN_LNC1()),\n",
    "    (\"GEN-REG\", GEN_REG()),\n",
    "    (\"GEN-AND\", GEN_AND()),\n",
    "    (\"GEN-DIV\", GEN_DIV()),\n",
    "    (\"GEN-STMC1\", GEN_STMC1()),\n",
    "    (\"GEN-STMC2\", GEN_STMC2()),\n",
    "    (\"GEN-PROX1\", GEN_PROX1()),\n",
    "    (\"GEN-PROX2\", GEN_PROX2()),\n",
    "    (\"GEN-PROX3\", GEN_PROX3()),\n",
    "    (\"GEN-PROX4\", GEN_PROX4()),\n",
    "    (\"GEN-PROX5\", GEN_PROX5()),\n",
    "    (\"GEN-aSL\", GEN_aSL()),\n",
    "    (\"GEN-TF-LNC\", GEN_TF_LNC()),\n",
    "]\n",
    "rag_retrieval_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in rag_retrieval_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in tqdm(rag_retrieval_axioms, desc=\"Axioms\", unit=\"axiom\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New generation axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ent = SpacyEntitiesAspectExtraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generation_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput]]] = [\n",
    "    #\n",
    "    # Coherence axioms\n",
    "    (\"COH1-0.75\", COH1(margin_fraction=0.75)),\n",
    "    (\"COH2-0.75\", COH2(margin_fraction=0.75)),\n",
    "    #\n",
    "    # Coverage axioms\n",
    "    (\"COV1-SE-0.5\", COV1(aspect_extraction=spacy_ent, margin_fraction=0.5)),\n",
    "    (\"COV2-SE-0.5\", COV2(aspect_extraction=spacy_ent, margin_fraction=0.5)),\n",
    "    (\"COV3-SE-0.5\", COV3(aspect_extraction=spacy_ent, margin_fraction=0.5)),\n",
    "    #\n",
    "    # Consistency axioms\n",
    "    (\"CONS1-SE-0.5\", CONS1(aspect_extraction=spacy_ent, margin_fraction=0.5)),\n",
    "    (\"CONS2-0.5\", CONS2(margin_fraction=0.5)),\n",
    "    (\"CONS3-0.5\", CONS3(margin_fraction=0.5)),\n",
    "    #\n",
    "    # Correctness axioms\n",
    "    (\"CORR1-0.75\", CORR1(margin_fraction=0.75)),\n",
    "    #\n",
    "    # Clarity axioms\n",
    "    (\"CLAR1-0.5\", CLAR1(margin_fraction=0.5)),\n",
    "    (\"CLAR2-0.5\", CLAR2(margin_fraction=0.5)),\n",
    "]\n",
    "rag_generation_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in rag_generation_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generation_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in tqdm(rag_generation_axioms, desc=\"Axioms\", unit=\"axiom\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_axioms = [\n",
    "    (\n",
    "        f\"VOTE-{axiomset_name}-{minimum_votes}\",\n",
    "        MajorityVoteAxiom(\n",
    "            axioms=[axiom for _, axiom in axiom_set],\n",
    "            minimum_votes=minimum_votes,\n",
    "        ),\n",
    "    )\n",
    "    for minimum_votes in [\n",
    "        0.0,\n",
    "        0.05,\n",
    "        0.1,\n",
    "        0.15,\n",
    "        0.2,\n",
    "        0.225,\n",
    "        0.25,\n",
    "        0.275,\n",
    "        0.3,\n",
    "        0.325,\n",
    "        0.35,\n",
    "        0.375,\n",
    "        0.4,\n",
    "        0.425,\n",
    "        0.45,\n",
    "        0.475,\n",
    "        0.5,\n",
    "    ]\n",
    "    for axiomset_name, axiom_set in [\n",
    "        (\"old\", rag_retrieval_axioms),\n",
    "        (\"new\", rag_generation_axioms),\n",
    "        (\"all\", rag_retrieval_axioms + rag_generation_axioms),\n",
    "    ]\n",
    "]\n",
    "mv_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in mv_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in tqdm(mv_axioms, desc=\"Axioms\", unit=\"axiom\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORIG Axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_axioms = [\n",
    "    (\n",
    "        f\"ORIG-{model.replace('/', '-')}\", TrecRagLlmOrigAxiom(\n",
    "            ratings_path = Path(f\"../data/ratings-{model.replace('/', '-')}.jsonl\")\n",
    "        ),\n",
    "        )\n",
    "    for model in [\n",
    "        \"HuggingFaceTB/SmolLM-135M-Instruct\",\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    ]\n",
    "    if Path(f\"../data/ratings-{model.replace('/', '-')}.jsonl\").exists()\n",
    "]\n",
    "orig_axioms = [\n",
    "    (name, axiom.cached(cache_path=cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in orig_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in tqdm(orig_axioms, desc=\"Axioms\", unit=\"axiom\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here we count the number matches for all types of preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for oracle_name, oracle_data_run_type, oracle_preferences in rag_oracle_prefs:\n",
    "    # Normalize preferences to -1, 0, 1.\n",
    "    oracle_preferences = sign(oracle_preferences)\n",
    "\n",
    "    for axiom_type, axiom_prefs in (\n",
    "        (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "        (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "        (\"aggregated axiom\", mv_axiom_prefs),\n",
    "        (\"orig axiom\", orig_axiom_prefs),\n",
    "    ):\n",
    "\n",
    "        for axiom_name, axiom_data_run_type, axiom_preferences in axiom_prefs:\n",
    "            # Normalize preferences to -1, 0, 1.\n",
    "            axiom_preferences = sign(axiom_preferences)\n",
    "\n",
    "            if oracle_data_run_type != axiom_data_run_type:\n",
    "                continue\n",
    "\n",
    "            for oracle_preference in (-1, 0, 1):\n",
    "                for axiom_preference in (-1, 0, 1):\n",
    "\n",
    "                    matching_preferences = (oracle_preferences == oracle_preference) & (\n",
    "                        axiom_preferences == axiom_preference\n",
    "                    )\n",
    "\n",
    "                    count = matching_preferences.sum()\n",
    "\n",
    "                    # Account for trivial matches along the diagonal.\n",
    "                    if oracle_preference == 0 and axiom_preference == 0:\n",
    "                        count -= oracle_preferences.shape[-1]\n",
    "\n",
    "                    df_data.append(\n",
    "                        {\n",
    "                            \"data_run_type\": oracle_data_run_type,\n",
    "                            \"oracle_name\": oracle_name,\n",
    "                            \"axiom_name\": axiom_name,\n",
    "                            \"axiom_type\": axiom_type,\n",
    "                            \"oracle_preference\": oracle_preference,\n",
    "                            \"axiom_preference\": axiom_preference,\n",
    "                            \"count\": count,\n",
    "                        }\n",
    "                    )\n",
    "df_distribution = DataFrame(df_data)\n",
    "# df_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = (\n",
    "    df_distribution\n",
    "    .groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\"])[[\"count\"]]\n",
    "    .sum()\n",
    "    .rename(columns={\"count\": \"total_count\"})\n",
    ")\n",
    "# total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute decisiveness of traditional ORACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_dl_2020 = get_dataset(\"irds:msmarco-passage/trec-dl-2020\")\n",
    "trec_dl_2020_qrels = trec_dl_2020.get_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_dl_2020_result_dir = Path(\"../data/runs/msmarco-passage/trec-dl-2020/\")\n",
    "trec_dl_2020_result_files = list(trec_dl_2020_result_dir.iterdir())\n",
    "trec_dl_2020_results = concat([\n",
    "    read_results(result_file)\n",
    "    for result_file in tqdm(trec_dl_2020_result_files, desc=\"Load results\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_dl_2020_results2 = trec_dl_2020_results.copy()\n",
    "trec_dl_2020_results2 = trec_dl_2020_results2[trec_dl_2020_results2[\"rank\"] < 5]\n",
    "trec_dl_2020_results2 = trec_dl_2020_results2[['qid', 'docno', 'name']]\n",
    "trec_dl_2020_results2 = trec_dl_2020_results2.merge(trec_dl_2020_qrels[['qid', 'docno', 'label']], how=\"left\", on=[\"qid\", \"docno\"])\n",
    "trec_dl_2020_results2 = trec_dl_2020_results2.merge(trec_dl_2020_results2, suffixes=(\"_a\", \"_b\"), on=[\"qid\", \"name\"])\n",
    "trec_dl_2020_results2[\"decisive\"] = trec_dl_2020_results2[\"label_a\"] != trec_dl_2020_results2[\"label_b\"]\n",
    "# trec_dl_2020_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_dl_2020_decisiveness = trec_dl_2020_results2[\"decisive\"].mean()\n",
    "# trec_dl_2020_decisiveness = 0.8381479690522243  # in case the above cell is not run\n",
    "trec_dl_2020_decisiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axiom decisiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decisiveness = df_distribution.copy()\n",
    "df_decisiveness = df_decisiveness[df_decisiveness[\"axiom_preference\"] != 0]\n",
    "df_decisiveness = (\n",
    "    df_decisiveness.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\", \"axiom_type\"])[[\"count\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"count\": \"non_zero_count\"})\n",
    ")\n",
    "df_decisiveness = df_decisiveness.merge(total_counts, on=[\"data_run_type\", \"oracle_name\", \"axiom_name\"])\n",
    "\n",
    "df_decisiveness = df_decisiveness.groupby(by=list(set(df_decisiveness.columns) - {\"oracle_name\"})).first().reset_index().drop(columns=\"oracle_name\")\n",
    " \n",
    "df_decisiveness[\"zero_count\"] = (\n",
    "    df_decisiveness[\"total_count\"] - df_decisiveness[\"non_zero_count\"]\n",
    ")\n",
    "df_decisiveness[\"decisiveness\"] = (\n",
    "    df_decisiveness[\"non_zero_count\"] / df_decisiveness[\"total_count\"]\n",
    ")\n",
    "\n",
    "df_decisiveness = df_decisiveness[\n",
    "    [\"data_run_type\", \"axiom_name\", \"axiom_type\", \"zero_count\", \"non_zero_count\", \"decisiveness\"]\n",
    "]\n",
    "\n",
    "# df_decisiveness.sort_values(\n",
    "#     [\"data_run_type\", \"decisiveness\"],\n",
    "#     ascending=[True, False],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decisiveness_oracle = df_distribution.copy()\n",
    "df_decisiveness_oracle = df_decisiveness_oracle[df_decisiveness_oracle[\"oracle_preference\"] != 0]\n",
    "df_decisiveness_oracle = (\n",
    "    df_decisiveness_oracle.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\", \"axiom_type\"])[[\"count\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"count\": \"non_zero_count\"})\n",
    ")\n",
    "df_decisiveness_oracle = df_decisiveness_oracle.merge(total_counts, on=[\"data_run_type\", \"oracle_name\", \"axiom_name\"])\n",
    "\n",
    "df_decisiveness_oracle = df_decisiveness_oracle.groupby(by=list(set(df_decisiveness_oracle.columns) - {\"axiom_name\", \"axiom_type\"})).first().reset_index().drop(columns=[\"axiom_name\", \"axiom_type\"])\n",
    "\n",
    "df_decisiveness_oracle[\"zero_count\"] = (\n",
    "    df_decisiveness_oracle[\"total_count\"] - df_decisiveness_oracle[\"non_zero_count\"]\n",
    ")\n",
    "df_decisiveness_oracle[\"decisiveness\"] = (\n",
    "    df_decisiveness_oracle[\"non_zero_count\"] / df_decisiveness_oracle[\"total_count\"]\n",
    ")\n",
    "\n",
    "df_decisiveness_oracle = df_decisiveness_oracle[\n",
    "    [\"data_run_type\", \"oracle_name\", \"zero_count\", \"non_zero_count\", \"decisiveness\"]\n",
    "]\n",
    "\n",
    "df_decisiveness_oracle.sort_values(\n",
    "    [\"data_run_type\", \"decisiveness\"],\n",
    "    ascending=[True, False],\n",
    ")\n",
    "\n",
    "df_decisiveness_oracle[\"margin\"] = df_decisiveness_oracle[\"oracle_name\"].str.extract(r\"-(\\d+\\.\\d+)$\").astype(float).fillna(0.0)\n",
    "df_decisiveness_oracle[\"target\"] = abs(df_decisiveness_oracle[\"decisiveness\"] - trec_dl_2020_decisiveness)\n",
    "\n",
    "# df_decisiveness_oracle.drop(columns=\"oracle_name\").groupby([\"data_run_type\", \"margin\"]).mean().sort_values([\"data_run_type\", \"target\"])\n",
    "# df_decisiveness_oracle.sort_values([\"data_run_type\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axiom consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consistency_relaxed = df_distribution.copy()\n",
    "df_consistency_relaxed = df_consistency_relaxed[df_consistency_relaxed[\"axiom_preference\"] != 0]\n",
    "\n",
    "# Case 1: A preference pair of 0 and 1 is consistent (must not contradict).\n",
    "df_consistency_relaxed = df_consistency_relaxed[(df_consistency_relaxed[\"axiom_preference\"] - df_consistency_relaxed[\"oracle_preference\"]).abs() <= 1]\n",
    "\n",
    "df_consistency_relaxed = df_consistency_relaxed.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\"])[[\"count\"]].sum().reset_index().rename(columns={\"count\": \"consistent_count\"})\n",
    "\n",
    "df_consistency_relaxed = df_consistency_relaxed.merge(df_decisiveness, on=[\"data_run_type\",\"axiom_name\"])\n",
    "df_consistency_relaxed[\"consistency\"] = df_consistency_relaxed[\"consistent_count\"] / df_consistency_relaxed[\"non_zero_count\"]\n",
    "df_consistency_relaxed[\"consistency\"] = df_consistency_relaxed[\"consistency\"]\n",
    "\n",
    "# df_consistency_relaxed.sort_values(\n",
    "#     [\"data_run_type\", \"oracle_name\", \"consistency\"],\n",
    "#     ascending=[True, True, False],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consistency_strict = df_distribution.copy()\n",
    "df_consistency_strict = df_consistency_strict[df_consistency_strict[\"axiom_preference\"] != 0]\n",
    "\n",
    "# Case 2: A preference pair of 0 and 1 is not consistent (must exactly match).\n",
    "df_consistency_strict = df_consistency_strict[df_consistency_strict[\"axiom_preference\"] == df_consistency_strict[\"oracle_preference\"]]\n",
    "\n",
    "df_consistency_strict = df_consistency_strict.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\"])[[\"count\"]].sum().reset_index().rename(columns={\"count\": \"consistent_count_strict\"})\n",
    "\n",
    "df_consistency_strict = df_consistency_strict.merge(df_decisiveness, on=[\"data_run_type\",\"axiom_name\"])\n",
    "df_consistency_strict[\"consistency_strict\"] = df_consistency_strict[\"consistent_count_strict\"] / df_consistency_strict[\"non_zero_count\"]\n",
    "df_consistency_strict[\"consistency_strict\"] = df_consistency_strict[\"consistency_strict\"]\n",
    "\n",
    "# df_consistency_strict.sort_values(\n",
    "#     [\"data_run_type\", \"oracle_name\", \"consistency_strict\"],\n",
    "#     ascending=[True, True, False],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consistency = df_consistency_relaxed.merge(df_consistency_strict, on=[\"data_run_type\", \"oracle_name\", \"axiom_name\", \"axiom_type\", \"zero_count\", \"non_zero_count\", \"decisiveness\"])\n",
    "# df_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_consistency.groupby([\"data_run_type\", \"oracle_name\"])[[\"consistency\", \"consistency_strict\"]].mean().reset_index().sort_values(\"consistency\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined axiom effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_effectiveness = df_consistency.copy()\n",
    "\n",
    "# Harmonic mean of decisiveness and consistency\n",
    "df_effectiveness[\"effectiveness\"] = 2 * (df_effectiveness[\"decisiveness\"] * df_effectiveness[\"consistency\"]) / (df_effectiveness[\"decisiveness\"] + df_effectiveness[\"consistency\"])\n",
    "\n",
    "# df_effectiveness.sort_values(\n",
    "#     [\"data_run_type\", \"oracle_name\", \"effectiveness\"],\n",
    "#     ascending=[True, True, False],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_effectiveness.groupby([\"data_run_type\", \"oracle_name\"])[[\"decisiveness\",\"consistency\", \"effectiveness\"]].mean().sort_values(by=[\"data_run_type\", \"effectiveness\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_effectiveness.groupby([\"data_run_type\", \"axiom_name\", \"axiom_type\"])[[\"decisiveness\",\"consistency\", \"effectiveness\"]].mean().sort_values(by=[\"data_run_type\", \"axiom_type\", \"effectiveness\"], ascending=[True, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional axiom effectiveness on TREC DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trad = read_csv(\"../data/traditional-axiom-effectiveness.csv\")\n",
    "df_trad.rename(columns={\"axiom\": \"axiom_name\"}, inplace=True)\n",
    "df_trad[\"axiom_name\"] = \"GEN-\" + df_trad[\"axiom_name\"]\n",
    "df_trad = df_trad[df_trad[\"dataset\"] == \"TREC 2020 DL\"]\n",
    "df_trad[\"decisiveness\"] = (df_trad[\"preference_eq_orig\"] + df_trad[\"preference_neq_orig\"]) / (\n",
    "    df_trad[\"preference_eq_orig\"] + df_trad[\"preference_neq_orig\"] + df_trad[\"preference_zero\"]\n",
    ")\n",
    "df_trad[\"consistency\"] = df_trad[\"consistency\"] / 100\n",
    "df_trad.drop(columns=[\"preference_zero\", \"preference_eq_orig\", \"preference_neq_orig\", \"dataset\"], inplace=True)\n",
    "\n",
    "# Harmonic mean of decisiveness and consistency\n",
    "df_trad[\"effectiveness\"] = 2 * (df_trad[\"decisiveness\"] * df_trad[\"consistency\"]) / (df_trad[\"decisiveness\"] + df_trad[\"consistency\"])\n",
    "\n",
    "df_trad[\"axiom_type\"] = \"adapted retrieval axiom\"\n",
    "df_trad = df_trad.merge(df_effectiveness[[\"axiom_name\", \"axiom_type\"]].drop_duplicates(), on=[\"axiom_name\"], how=\"right\", suffixes=(\"_trad\", \"\"))\n",
    "df_trad.drop(columns=[\"axiom_type_trad\"], inplace=True)\n",
    "\n",
    "df_trad[\"oracle_name\"] = \"Qrels\"\n",
    "df_trad[\"data_run_type\"] = \"traditional\"\n",
    "\n",
    "# df_trad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables for axiom effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_margin_fraction = \"\"\n",
    "# oracle_margin_fraction = \"-0.1\"\n",
    "oracle_display_names = {\n",
    "    f\"ORACLE-NUGGET-ALL{oracle_margin_fraction}\": r\"$\\Nugg_A$\",\n",
    "    # f\"ORACLE-NUGGET-ALL-STRICT{oracle_margin_fraction}\": r\"$\\Nugg_{A,\\text{strict}}$\",\n",
    "    f\"ORACLE-NUGGET-VITAL{oracle_margin_fraction}\": r\"$\\Nugg_V$\",\n",
    "    # f\"ORACLE-NUGGET-VITAL-STRICT{oracle_margin_fraction}\": r\"$\\Nugg_{V,\\text{strict}}$\",\n",
    "    f\"ORACLE-NUGGET-WEIGHTED{oracle_margin_fraction}\": r\"$\\Nugg_W$\",\n",
    "    # f\"ORACLE-NUGGET-WEIGHTED-STRICT{oracle_margin_fraction}\": r\"$\\Nugg_{W,\\text{strict}}$\",\n",
    "    f\"ORACLE-CROWD-OVERALL{oracle_margin_fraction}\": r\"Qual.\",\n",
    "    f\"ORACLE-CROWD-STYLISTIC-COHERENCE{oracle_margin_fraction}\": r\"\\Coh_S$\",\n",
    "    f\"ORACLE-CROWD-LOGICAL-COHERENCE{oracle_margin_fraction}\": r\"$\\Coh_L$\",\n",
    "    f\"ORACLE-CROWD-BROAD-COVERAGE{oracle_margin_fraction}\": r\"$\\Cov_B$\",\n",
    "    f\"ORACLE-CROWD-DEEP-COVERAGE{oracle_margin_fraction}\": r\"$\\Cov_D$\",\n",
    "    f\"ORACLE-CROWD-INTERNAL-CONSISTENCY{oracle_margin_fraction}\": r\"$\\Cons_I$\",\n",
    "    f\"ORACLE-CROWD-TOPICAL-CORRECTNESS{oracle_margin_fraction}\": r\"$\\Corr_T$\",\n",
    "    \"Qrels\": r\"Qrels\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run_type_display_names = {\n",
    "    \"crowd\": r\"Crowd RAG Corpus 2025\",\n",
    "    \"assignments\": r\"TREC 2024 RAG Official\",\n",
    "    \"traditional\": r\"TREC 2020 DL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in df_effectiveness[\"axiom_name\"].drop_duplicates():\n",
    "#     if name.startswith(\"GEN-\"):\n",
    "#         continue\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axiom_display_names = {\n",
    "    # Adapted retrieval axioms\n",
    "    \"GEN-TFC1\": \"TFC1\",\n",
    "    \"GEN-TFC3\": \"TFC3\",\n",
    "    \"GEN-M-TDC\": \"M-TDC\",\n",
    "    \"GEN-LNC1\": \"LNC1\",\n",
    "    \"GEN-TF-LNC\": \"TF-LNC\",\n",
    "    \"GEN-LB1\": \"LB1\",\n",
    "    \"GEN-REG\": \"REG\",\n",
    "    \"GEN-AND\": \"AND\",\n",
    "    \"GEN-DIV\": \"DIV\",\n",
    "    \"GEN-STMC1\": \"STMC1\",\n",
    "    \"GEN-STMC2\": \"STMC2\",\n",
    "    \"GEN-PROX1\": \"PROX1\",\n",
    "    \"GEN-PROX2\": \"PROX2\",\n",
    "    \"GEN-PROX3\": \"PROX3\",\n",
    "    \"GEN-PROX4\": \"PROX4\",\n",
    "    \"GEN-PROX5\": \"PROX5\",\n",
    "    \"GEN-ArgUC\": \"ArgUC\",\n",
    "    \"GEN-QTArg\": \"QTArg\",\n",
    "    \"GEN-QTPArg\": \"QTPArg\",\n",
    "    \"GEN-aSL\": \"aSL\",\n",
    "    #\n",
    "    # Coherence axioms\n",
    "    \"COH1-0.75\": \"COH1 \",\n",
    "    \"COH2-0.75\": \"COH2 \",\n",
    "    #\n",
    "    # Coverage axioms\n",
    "    \"COV1-SE-0.5\": \"COV1 \",\n",
    "    \"COV2-SE-0.5\": \"COV2 \",\n",
    "    \"COV3-SE-0.5\": \"COV3 \",\n",
    "    #\n",
    "    # Consistency axioms\n",
    "    \"CONS1-SE-0.5\": \"CONS1\",\n",
    "    \"CONS2-0.5\": \"CONS2\",\n",
    "    \"CONS3-0.5\": \"CONS3\",\n",
    "    #\n",
    "    # Correctness axioms\n",
    "    \"CORR1-0.75\": \"CORR1\",\n",
    "    #\n",
    "    # Clarity axioms\n",
    "    \"CLAR1-0.5\": \"CLAR1\",\n",
    "    \"CLAR2-0.5\": \"CLAR2\",\n",
    "    #\n",
    "    # Vote axioms\n",
    "    \"VOTE-old-0.0\": r\"VOTE\\textsubscript{trad,\\,0\\%}\",\n",
    "    \"VOTE-new-0.0\": r\"VOTE\\textsubscript{new,\\,0\\%}\",\n",
    "    #\n",
    "    # ORIG axioms\n",
    "    \"ORIG-HuggingFaceTB-SmolLM-135M-Instruct\": r\"ORIG\\textsubscript{SmolLM}\",\n",
    "    \"ORIG-Qwen-Qwen2.5-0.5B-Instruct\": r\"ORIG\\textsubscript{Qwen2.5}\",\n",
    "    \"ORIG-TinyLlama-TinyLlama-1.1B-Chat-v1.0\": r\"ORIG\\textsubscript{TinyLlama}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df_effectiveness.copy()\n",
    "df_table = concat([df_table, df_trad])\n",
    "\n",
    "df_table[\"axiom_name\"] = Categorical(\n",
    "    df_table[\"axiom_name\"].replace(axiom_display_names),\n",
    "    list(axiom_display_names.values()),\n",
    ")\n",
    "df_table[\"oracle_name\"] = Categorical(\n",
    "    df_table[\"oracle_name\"].replace(oracle_display_names), list(oracle_display_names.values())\n",
    ")\n",
    "df_table[\"data_run_type\"] = Categorical(\n",
    "    df_table[\"data_run_type\"].replace(data_run_type_display_names), list(data_run_type_display_names.values())\n",
    ")\n",
    "\n",
    "# df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table() -> None:\n",
    "    columns = [\"l\"]\n",
    "    for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            columns += [\"r\"]\n",
    "        columns += [\"r\"]\n",
    "    print(r\"\\begin{tabular}{@{}\" + \"\".join(columns) + r\"@{}}\")\n",
    "    print(r\"  \\toprule\")\n",
    "\n",
    "    columns = [r\"\\textbf{Axiom}\"]\n",
    "    for data_run_type, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        num_columns = 1\n",
    "        for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            num_columns += 1\n",
    "        columns += [\n",
    "            r\"\\multicolumn{\"\n",
    "            + f\"{num_columns}\"\n",
    "            + r\"}{c}{\\textbf{\"\n",
    "            + data_run_type\n",
    "            + r\"}}\"\n",
    "        ]\n",
    "    print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "    columns = [\"\"]\n",
    "    i = 2\n",
    "    for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        start = i\n",
    "        i += 1\n",
    "        for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            i += 1\n",
    "        columns += [r\"\\cmidrule(lr){\" + f\"{start}\" + r\"-\" + f\"{i-1}\" + r\"}\"]\n",
    "    print(r\"  \" + r\"\".join(columns))\n",
    "\n",
    "    columns = [\"\"]\n",
    "    for data_run_type, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        num_columns = 0\n",
    "        for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            num_columns += 1\n",
    "        consistency_name = (\n",
    "            \"Consistency\"\n",
    "            if data_run_type != data_run_type_display_names[\"traditional\"]\n",
    "            else \"Cons.\"\n",
    "        )\n",
    "        columns += [\n",
    "            r\"\\multicolumn{\"\n",
    "            + f\"{num_columns}\"\n",
    "            + r\"}{c}{\\textbf{\"\n",
    "            + consistency_name\n",
    "            + r\"}}\"\n",
    "        ]\n",
    "        columns += [r\"\\textbf{Dec.}\"]\n",
    "    print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "    columns = [\"\"]\n",
    "    i = 2\n",
    "    for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        start = i\n",
    "        for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            i += 1\n",
    "        columns += [r\"\\cmidrule(lr){\" + f\"{start}\" + r\"-\" + f\"{i-1}\" + r\"}\"]\n",
    "        columns += [r\"\\cmidrule(lr){\" + f\"{i}\" + r\"-\" + f\"{i}\" + r\"}\"]\n",
    "        i += 1\n",
    "    print(r\"  \" + r\"\".join(columns))\n",
    "\n",
    "    columns = [\"\"]\n",
    "    for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "        if len(df_data_run_type) == 0:\n",
    "            continue\n",
    "        for oracle_name, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "            if len(df_oracle) == 0:\n",
    "                continue\n",
    "            columns += [r\"\\multicolumn{1}{c}{\" + oracle_name + r\"}\"]\n",
    "        columns += [\"\"]\n",
    "    print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "    for _, df_axiom_type in df_table.sort_values(\"axiom_name\").groupby(\n",
    "        \"axiom_type\", sort=False\n",
    "    ):\n",
    "        if len(df_axiom_type) <= 0:\n",
    "            continue\n",
    "        print(r\"  \\midrule\")\n",
    "        for axiom_name, df_axiom in df_axiom_type.groupby(\"axiom_name\"):\n",
    "            if len(df_axiom) <= 0:\n",
    "                continue\n",
    "            columns = [axiom_name.removeprefix(\"GEN-\")]\n",
    "            for data_run_type, df_data_run_type in df_axiom.groupby(\"data_run_type\"):\n",
    "                if len(df_data_run_type) == 0:\n",
    "                    continue\n",
    "                if len(df_data_run_type[\"decisiveness\"].unique()) != 1:\n",
    "                    raise ValueError()\n",
    "                decisiveness = df_data_run_type.iloc[0][\"decisiveness\"]\n",
    "                decisive = not isnan(decisiveness) and decisiveness >= 0.005\n",
    "                decisiveness_prefix = \"\" if decisive else r\"\\color{gray} \"\n",
    "                for oracle_name, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "                    oracle_prefix = (\n",
    "                        r\"\\multicolumn{1}{>{\\columncolor{tablegray}[0.5\\tabcolsep]}r}{\"\n",
    "                        if axiom_name.removeprefix(\"GEN-\")[:3].casefold()\n",
    "                        in oracle_name.casefold()\n",
    "                        or (\"VOTE\" in axiom_name and \"OVERALL\" in oracle_name)\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    oracle_suffix = (\n",
    "                        r\"}\"\n",
    "                        if axiom_name.removeprefix(\"GEN-\")[:3].casefold()\n",
    "                        in oracle_name.casefold()\n",
    "                        or (\"VOTE\" in axiom_name and \"OVERALL\" in oracle_name)\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    if len(df_oracle) == 0:\n",
    "                        continue\n",
    "                    if len(df_oracle) != 1:\n",
    "                        raise ValueError()\n",
    "                    row = df_oracle.iloc[0]\n",
    "                    consistency = row[\"consistency\"]\n",
    "                    columns += [\n",
    "                        oracle_prefix\n",
    "                        + (decisiveness_prefix if consistency > 0.5 else r\"\\color{gray} \")\n",
    "                        + (\n",
    "                            (\n",
    "                                f\"{consistency:4.0%}\".replace(\"%\", r\"\\%\")\n",
    "                                if decisiveness > 0\n",
    "                                else \"  ---\"\n",
    "                            )\n",
    "                            if not isnan(consistency) and decisive\n",
    "                            else \"  ---\"\n",
    "                        )\n",
    "                        + oracle_suffix\n",
    "                    ]\n",
    "                columns += [\n",
    "                    (\n",
    "                        (\n",
    "                            f\"{decisiveness:4.0%}\".replace(\"%\", r\"\\%\")\n",
    "                            if not isnan(decisiveness)\n",
    "                            else \"  ---\"\n",
    "                        )\n",
    "                    ),\n",
    "                ]\n",
    "            print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "    print(r\"  \\bottomrule\")\n",
    "    print(r\"\\end{tabular}\")\n",
    "\n",
    "\n",
    "print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Can we emulate some manual preference judgments by voting axioms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs = df_effectiveness.copy()\n",
    "df_cs = df_cs[~df_cs[\"oracle_name\"].str.contains(\"0.\")]\n",
    "df_cs = df_cs[df_cs[\"oracle_name\"].str.startswith(\"ORACLE-CROWD-OVERALL\")]\n",
    "df_cs = df_cs[df_cs[\"axiom_name\"].str.startswith(\"VOTE\")]\n",
    "df_cs[\"vote_threshold\"] = [float(axiom_name.split(\"-\")[2]) for axiom_name in df_cs[\"axiom_name\"]]\n",
    "df_cs[\"vote_type\"] = [axiom_name.split(\"-\")[1] for axiom_name in df_cs[\"axiom_name\"]]\n",
    "df_cs[\"consistency\"] = df_cs[\"consistency\"].fillna(1)\n",
    "df_cs[\"consistency_strict\"] = df_cs[\"consistency_strict\"].fillna(1)\n",
    "df_cs[\"global_consistency\"] = df_cs[\"consistency\"] * df_cs[\"decisiveness\"] + 1 - df_cs[\"decisiveness\"]\n",
    "df_cs[\"global_consistency_strict\"] = df_cs[\"consistency_strict\"] * df_cs[\"decisiveness\"] + 1 - df_cs[\"decisiveness\"]\n",
    "df_cs[\"human_judgment_effort\"] = 1 - df_cs[\"decisiveness\"]\n",
    "df_cs = df_cs.drop(columns=[\"axiom_name\", \"axiom_type\", \"oracle_name\", \"data_run_type\", \"consistent_count\", \"non_zero_count\", \"zero_count\", \"consistent_count_strict\", \"effectiveness\"])\n",
    "df_cs = df_cs.reset_index(drop=True)\n",
    "df_cs = concat([df_cs, DataFrame([{ \n",
    "    \"vote_threshold\": 0,\n",
    "    \"vote_type\": \"random\",\n",
    "    \"decisiveness\": 0,\n",
    "    \"consistency\": 0.5,\n",
    "    \"consistency_strict\": 0.5,\n",
    "    \"global_consistency\": 1,\n",
    "    \"global_consistency_strict\": 1,\n",
    "    \"human_judgment_effort\": 1,\n",
    "},{\n",
    "    \"vote_threshold\": 0,\n",
    "    \"vote_type\": \"random\",\n",
    "    \"decisiveness\": 1,\n",
    "    \"consistency\": 0.5,\n",
    "    \"consistency_strict\": 0.5,\n",
    "    \"global_consistency\": 0.5,\n",
    "    \"global_consistency_strict\": 0.5,\n",
    "    \"human_judgment_effort\": 0,\n",
    "}])])\n",
    "\n",
    "df_cs[\"vote_type\"] = Categorical(\n",
    "    df_cs[\"vote_type\"], [\"new\", \"all\", \"random\", \"old\"]\n",
    ")\n",
    "# df_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = color_palette()\n",
    "palette[2] = \"gray\"\n",
    "\n",
    "print(r\"\\definecolor{legendall}{rgb}{\" + f\"{palette[0][0]}\" + r\",\" + f\"{palette[0][1]}\" + r\",\" + f\"{palette[0][2]}\" + r\"}\")\n",
    "print(r\"\\definecolor{legendnew}{rgb}{\" + f\"{palette[1][0]}\" + r\",\" + f\"{palette[1][1]}\" + r\",\" + f\"{palette[1][2]}\" + r\"}\")\n",
    "print(r\"\\definecolor{legendrandom}{gray}{0.5}\")\n",
    "print(r\"\\definecolor{legendold}{rgb}{\" + f\"{palette[3][0]}\" + r\",\" + f\"{palette[3][1]}\" + r\",\" + f\"{palette[3][2]}\" + r\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = color_palette()\n",
    "palette[2] = \"gray\"\n",
    "plot = FacetGrid(\n",
    "    data=df_cs,\n",
    "    margin_titles=True,\n",
    "    xlim=(0, 0.775),\n",
    "    ylim=(0.45, 1),\n",
    ")\n",
    "plot.map_dataframe(\n",
    "    lineplot,\n",
    "    x=\"decisiveness\",\n",
    "    y=\"consistency\",\n",
    "    hue=\"vote_type\",\n",
    "    style=\"vote_type\",\n",
    "    markers=True,\n",
    "    palette=palette,\n",
    ")\n",
    "for (vote_type, df_vote_type), color in zip(\n",
    "    df_cs.groupby(\"vote_type\"), color_palette()\n",
    "):\n",
    "    if vote_type == \"random\":\n",
    "        continue\n",
    "    if vote_type == \"new\":\n",
    "        vote_thresholds = [\n",
    "            0.0,\n",
    "            0.1,\n",
    "            0.2,\n",
    "            0.3,\n",
    "            1.0,\n",
    "        ]\n",
    "    elif vote_type == \"all\":\n",
    "        vote_thresholds = [\n",
    "            0.0,\n",
    "            0.1,\n",
    "            0.15,\n",
    "            0.2,\n",
    "            1.0,\n",
    "        ]\n",
    "    elif vote_type == \"old\":\n",
    "        vote_thresholds = [\n",
    "            0.0,\n",
    "            0.2,\n",
    "            0.25,\n",
    "            0.3,\n",
    "        ]\n",
    "    for x, y, vote_threshold in df_vote_type[\n",
    "        [\"decisiveness\", \"consistency\", \"vote_threshold\"]\n",
    "    ].values:\n",
    "        if vote_threshold not in vote_thresholds:\n",
    "            continue\n",
    "\n",
    "        if vote_type == \"new\":\n",
    "            verticalalignment = \"bottom\"\n",
    "            horizontalalignment = \"left\"\n",
    "            offsetx = 0.01\n",
    "            offsety = 0.01\n",
    "        elif vote_type == \"all\":\n",
    "            verticalalignment = \"top\"\n",
    "            horizontalalignment = \"center\"\n",
    "            offsetx = 0.0\n",
    "            offsety = -0.02\n",
    "            if vote_threshold == 0.1:\n",
    "                horizontalalignment = \"right\"\n",
    "                offsety = -0.01\n",
    "        elif vote_type == \"old\":\n",
    "            verticalalignment = \"top\"\n",
    "            horizontalalignment = \"center\"\n",
    "            offsetx = 0.0\n",
    "            offsety = -0.03\n",
    "            if vote_threshold == 0.25:\n",
    "                offsetx = 0.02\n",
    "                offsety = -0.025\n",
    "            elif vote_threshold == 0.3:\n",
    "                offsetx = 0.02\n",
    "                offsety = -0.02\n",
    "        plot.ax.text(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            s=f\"{vote_threshold:.0%}\" + (\" minimum votes\" if vote_type == \"new\" and vote_threshold == 0.3 else \"\"),\n",
    "            color=color,\n",
    "            horizontalalignment=horizontalalignment,\n",
    "            verticalalignment=verticalalignment,\n",
    "            position=(x + offsetx, y + offsety),\n",
    "        )\n",
    "\n",
    "plot.ax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "plot.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "plot.set_titles(\n",
    "    row_template=\"{row_name}\",\n",
    "    col_template=\"{col_name}\",\n",
    ")\n",
    "plot.set_axis_labels(\"Decisiveness\", \"Axiom Consistency\")\n",
    "plot.savefig(\"../data/figures/figure-vote-axiom-decisiveness-consistency.pdf\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = FacetGrid(\n",
    "    data=df_cs,\n",
    "    margin_titles=True,\n",
    "    xlim=(0.2, 1),\n",
    "    ylim=(0.72, 1),\n",
    ")\n",
    "plot.map_dataframe(\n",
    "    lineplot,\n",
    "    x=\"human_judgment_effort\",\n",
    "    y=\"global_consistency_strict\",\n",
    "    hue=\"vote_type\",\n",
    "    style=\"vote_type\",\n",
    "    palette=palette,\n",
    ")\n",
    "plot.refline(\n",
    "    y=0.90,\n",
    "    linestyle=\"--\",\n",
    "    color=\"black\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plot.set_titles(\n",
    "    row_template=\"{row_name}\",\n",
    "    col_template=\"{col_name}\",\n",
    ")\n",
    "plot.refline(\n",
    "    x=0.71,\n",
    "    color=\"black\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plot.ax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "plot.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "plot.set_axis_labels(\"Human Judgments\", \"Preferences Correct\")\n",
    "plot.savefig(\"../data/figures/figure-vote-axiom-judgment-consistency.pdf\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conflicting pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conf = []\n",
    "\n",
    "for oracle_name, oracle_data_run_type, oracle_preferences in rag_oracle_prefs:\n",
    "    # Normalize preferences to -1, 0, 1.\n",
    "    oracle_preferences = sign(oracle_preferences)\n",
    "\n",
    "    stacked_axiom_preferences = stack(\n",
    "        [\n",
    "            axiom_preferences\n",
    "            for _, axiom_prefs in (\n",
    "                (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "                (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "            )\n",
    "            for _, data_run_type, axiom_preferences in axiom_prefs\n",
    "            if oracle_data_run_type == data_run_type\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Count how many axioms would have predicted the oracle preference.\n",
    "    matching_axiom_preferences: NDArray = (\n",
    "        stacked_axiom_preferences == oracle_preferences\n",
    "    ).sum(axis=0)\n",
    "\n",
    "    for orig_name, orig_data_run_type, orig_preferences in orig_axiom_prefs:\n",
    "        if oracle_data_run_type != orig_data_run_type:\n",
    "            continue\n",
    "\n",
    "        # Normalize preferences to -1, 0, 1.\n",
    "        orig_preferences = sign(orig_preferences)\n",
    "\n",
    "        # Find conflicting pairs where the original ranking opposes the oracle ranking.\n",
    "        conflicting_preferences_mask = (\n",
    "            orig_preferences == (-1 * oracle_preferences)\n",
    "        ) & (orig_preferences != 0)\n",
    "\n",
    "        for input_index in range(orig_preferences.shape[0]):\n",
    "            for output_index1 in range(orig_preferences.shape[-2]):\n",
    "                for output_index2 in range(orig_preferences.shape[-1]):\n",
    "                    if not conflicting_preferences_mask[\n",
    "                        input_index, output_index1, output_index2\n",
    "                    ]:\n",
    "                        continue\n",
    "                    if matching_axiom_preferences[\n",
    "                        input_index, output_index1, output_index2\n",
    "                    ] == 0:\n",
    "                        continue\n",
    "                    data_conf.append(\n",
    "                        {\n",
    "                            \"data_run_type\": oracle_data_run_type,\n",
    "                            \"oracle_name\": oracle_name,\n",
    "                            \"axiom_name\": orig_name,\n",
    "                            \"input_index\": input_index,\n",
    "                            \"output_index1\": output_index1,\n",
    "                            \"output_index2\": output_index2,\n",
    "                            \"oracle_preference\": oracle_preferences[\n",
    "                                input_index, output_index1, output_index2\n",
    "                            ],\n",
    "                            \"orig_preference\": orig_preferences[\n",
    "                                input_index, output_index1, output_index2\n",
    "                            ],\n",
    "                            \"num_axiom_conflicts\": matching_axiom_preferences[\n",
    "                                input_index, output_index1, output_index2\n",
    "                            ],\n",
    "                        }\n",
    "                    )\n",
    "df_conf = DataFrame(data_conf)\n",
    "df_conf = df_conf.sort_values(\"num_axiom_conflicts\", ascending=False)\n",
    "# df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_data = to_inputs_outputs(read_rag_crowd_outputs_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_overal = df_conf.copy()\n",
    "df_conf_overal = df_conf_overal[df_conf_overal[\"oracle_name\"] == \"ORACLE-CROWD-OVERALL\"]\n",
    "df_conf_overal = df_conf_overal[df_conf_overal[\"num_axiom_conflicts\"] > 5]\n",
    "df_conf_overal[\"input\"] = [\n",
    "    conf_data[row[\"input_index\"]][0]\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal[\"input_text\"] = [\n",
    "    conf_data[row[\"input_index\"]][0].text\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal[\"output1\"] = [\n",
    "    conf_data[row[\"input_index\"]][1][row[\"output_index1\"]]\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal[\"output1_text\"] = [\n",
    "    conf_data[row[\"input_index\"]][1][row[\"output_index1\"]].text\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal[\"output2\"] = [\n",
    "    conf_data[row[\"input_index\"]][1][row[\"output_index2\"]]\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal[\"output2_text\"] = [\n",
    "    conf_data[row[\"input_index\"]][1][row[\"output_index2\"]].text\n",
    "    for _, row in df_conf_overal.iterrows()\n",
    "]\n",
    "df_conf_overal = df_conf_overal[df_conf_overal[\"orig_preference\"] == 1]\n",
    "df_conf_overal = df_conf_overal[~df_conf_overal[\"input_text\"].str.contains(\"citizen\")]\n",
    "# df_conf_overal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_example = df_conf_overal.iloc[1]\n",
    "# conf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_example_prefs = {}\n",
    "for _, axiom_prefs in (\n",
    "    (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "    (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "):\n",
    "    for axiom_name, _, axiom_preferences in axiom_prefs:\n",
    "        if axiom_name in (\"GEN-REG\", \"GEN-AND\", \"GEN-DIV\"):\n",
    "            continue\n",
    "        pref = axiom_preferences[conf_example[\"input_index\"]][conf_example[\"output_index1\"]][conf_example[\"output_index2\"]]\n",
    "        if pref != 0:\n",
    "            conf_example_prefs[axiom_display_names[axiom_name]] = pref\n",
    "            # print(axiom_name, pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_response(response: str) -> str:\n",
    "    response = (\n",
    "        response.replace(\"\\n\", \" \")\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\"%\", r\"\\%\")\n",
    "        .replace(\"$\", r\"\\$\")\n",
    "    )\n",
    "    response = r\"``\" + response + r\"''\"\n",
    "    response_words = response.split(\" \")\n",
    "    max_words = 38\n",
    "    if len(response_words) > max_words:\n",
    "        response = (\n",
    "            \" \".join(response_words[: int(max_words * 0.8)])\n",
    "            + r\" \\textcolor{gray}{[...]} \"\n",
    "            + \" \".join(response_words[int(-max_words * 0.2) :])\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [r\"l\", r\"p{\\responsewidth}\", r\"@{\\qquad}\"] + [r\"c\"] * len(conf_example_prefs)\n",
    "print(r\"\\begin{tabular}{@{}\" + \"\".join(columns) + r\"@{}}\")\n",
    "print(r\"  \\toprule\")\n",
    "columns = [\n",
    "    r\"\\multicolumn{2}{@{}p{\\responsewidth}}{\\textbf{Query} \\quad \" +\n",
    "    conf_example[\"input\"].id + r\" \\quad \\query{\" + conf_example[\"input\"].text + r\"}}\",\n",
    "] \n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\[-2.2ex]\")\n",
    "columns = [\n",
    "    r\"\\#\",\n",
    "    r\"\\textbf{Response}\",\n",
    "] + [r\"\\rotatebox{90}{\\parbox{\\axiomwidth}{\\centering\\textbf{\" + axiom_name.strip() + r\"}}}\" for axiom_name, _ in conf_example_prefs.items()]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "print(r\"  \\midrule\")\n",
    "columns = [\n",
    "    r\"1\",\n",
    "     _format_response(conf_example[\"output1\"].text) ,\n",
    "] + [\n",
    "    r\"$\\Uparrow$\" if pref > 0 else r\"$\\Downarrow$\"\n",
    "    for _, pref in conf_example_prefs.items()\n",
    "]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "columns = [\n",
    "    r\"2\",\n",
    "    _format_response(conf_example[\"output2\"].text),\n",
    "] + [\n",
    "    r\"$\\Uparrow$\" if pref < 0 else r\"$\\Downarrow$\"\n",
    "    for _, pref in conf_example_prefs.items()\n",
    "]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "print(r\"  \\bottomrule\")\n",
    "print(r\"\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-axiom consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_inter = []\n",
    "for axiom1_type, axiom1_prefs in (\n",
    "    (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "    (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "    (\"aggregated axiom\", mv_axiom_prefs),\n",
    "    (\"orig axiom\", orig_axiom_prefs),\n",
    "):\n",
    "    for axiom1_name, axiom1_data_run_type, axiom1_preferences in axiom1_prefs:\n",
    "        # Normalize preferences to -1, 0, 1.\n",
    "        axiom1_preferences = sign(axiom1_preferences)\n",
    "\n",
    "        for axiom2_type, axiom2_prefs in (\n",
    "            (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "            (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "            (\"aggregated axiom\", mv_axiom_prefs),\n",
    "            (\"orig axiom\", orig_axiom_prefs),\n",
    "        ):\n",
    "\n",
    "            for axiom2_name, axiom2_data_run_type, axiom2_preferences in axiom2_prefs:\n",
    "                # Normalize preferences to -1, 0, 1.\n",
    "                axiom2_preferences = sign(axiom2_preferences)\n",
    "\n",
    "                if axiom1_data_run_type != axiom2_data_run_type:\n",
    "                    continue\n",
    "\n",
    "                for axiom1_preference in (-1, 0, 1):\n",
    "                    for axiom2_preference in (-1, 0, 1):\n",
    "\n",
    "                        matching_preferences = (axiom1_preferences == axiom1_preference) & (\n",
    "                            axiom2_preferences == axiom2_preference\n",
    "                        )\n",
    "\n",
    "                        count = matching_preferences.sum()\n",
    "\n",
    "                        # Account for trivial matches along the diagonal.\n",
    "                        if axiom1_preference == 0 and axiom2_preference == 0:\n",
    "                            count -= axiom1_preferences.shape[-1]\n",
    "\n",
    "                        df_data_inter.append(\n",
    "                            {\n",
    "                                \"data_run_type\": axiom1_data_run_type,\n",
    "                                \"axiom1_name\": axiom1_name,\n",
    "                                \"axiom1_type\": axiom1_type,\n",
    "                                \"axiom2_name\": axiom2_name,\n",
    "                                \"axiom2_type\": axiom2_type,\n",
    "                                \"axiom1_preference\": axiom1_preference,\n",
    "                                \"axiom2_preference\": axiom2_preference,\n",
    "                                \"count\": count,\n",
    "                            }\n",
    "                        )\n",
    "df_distribution_inter = DataFrame(df_data_inter)\n",
    "# df_distribution_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consistency_inter = df_distribution_inter.copy()\n",
    "\n",
    "# df_consistency_inter = df_consistency_inter[df_consistency_inter[\"axiom1_preference\"] != 0]\n",
    "\n",
    "df_consistency_inter_count = df_consistency_inter.groupby([\"data_run_type\", \"axiom1_name\", \"axiom2_name\"])[[\"count\"]].sum().reset_index().rename(columns={\"count\": \"non_zero_count\"})\n",
    "\n",
    "# Case 1: A preference pair of 0 and 1 is consistent (must not contradict).\n",
    "df_consistency_inter = df_consistency_inter[(df_consistency_inter[\"axiom1_preference\"] - df_consistency_inter[\"axiom2_preference\"]).abs() <= 1]\n",
    "\n",
    "df_consistency_inter = df_consistency_inter.groupby([\"data_run_type\", \"axiom1_name\", \"axiom2_name\"])[[\"count\"]].sum().reset_index().rename(columns={\"count\": \"consistent_count\"})\n",
    "\n",
    "df_consistency_inter = df_consistency_inter.merge(df_consistency_inter_count, on=[\"data_run_type\", \"axiom1_name\", \"axiom2_name\"])\n",
    "\n",
    "df_consistency_inter[\"consistency\"] = df_consistency_inter[\"consistent_count\"] / df_consistency_inter[\"non_zero_count\"]\n",
    "df_consistency_inter[\"inconsistency\"] = 1 - df_consistency_inter[\"consistency\"]\n",
    "\n",
    "df_consistency_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap, set_theme, cubehelix_palette\n",
    "from numpy import tril, triu\n",
    "\n",
    "set_theme(\n",
    "    context=\"paper\",\n",
    "    style=\"white\",\n",
    ")\n",
    "\n",
    "min_consistency = df_consistency_inter[\"consistency\"].min()\n",
    "\n",
    "\n",
    "def df_heatmap(data: DataFrame, **kwargs):\n",
    "    df = data.copy()\n",
    "    df = df[\n",
    "        (\n",
    "            df[\"axiom1_name\"].str.startswith(\"COH\")\n",
    "            | df[\"axiom1_name\"].str.startswith(\"COV\")\n",
    "            | df[\"axiom1_name\"].str.startswith(\"CONS\")\n",
    "            | df[\"axiom1_name\"].str.startswith(\"CORR\")\n",
    "            | df[\"axiom1_name\"].str.startswith(\"CLAR\")\n",
    "            | df[\"axiom1_name\"].str.startswith(\"GEN\")\n",
    "        )\n",
    "        & (\n",
    "            df[\"axiom2_name\"].str.startswith(\"COH\")\n",
    "            | df[\"axiom2_name\"].str.startswith(\"COV\")\n",
    "            | df[\"axiom2_name\"].str.startswith(\"CONS\")\n",
    "            | df[\"axiom2_name\"].str.startswith(\"CORR\")\n",
    "            | df[\"axiom2_name\"].str.startswith(\"CLAR\")\n",
    "            | df[\"axiom2_name\"].str.startswith(\"GEN\")\n",
    "        )\n",
    "    ]\n",
    "    df[\"axiom1_name\"] = Categorical(\n",
    "        values=df[\"axiom1_name\"].map(axiom_display_names),\n",
    "        categories=[\n",
    "            v\n",
    "            for k, v in axiom_display_names.items()\n",
    "            if k.startswith(\"COH\")\n",
    "            or k.startswith(\"COV\")\n",
    "            or k.startswith(\"CONS\")\n",
    "            or k.startswith(\"CORR\")\n",
    "            or k.startswith(\"CLAR\")\n",
    "        ]\n",
    "        + [v for k, v in axiom_display_names.items() if k.startswith(\"GEN\")],\n",
    "    )\n",
    "    df[\"axiom2_name\"] = Categorical(\n",
    "        values=df[\"axiom2_name\"].map(axiom_display_names),\n",
    "        categories=[\n",
    "            v\n",
    "            for k, v in axiom_display_names.items()\n",
    "            if k.startswith(\"COH\")\n",
    "            or k.startswith(\"COV\")\n",
    "            or k.startswith(\"CONS\")\n",
    "            or k.startswith(\"CORR\")\n",
    "            or k.startswith(\"CLAR\")\n",
    "        ]\n",
    "        + [v for k, v in axiom_display_names.items() if k.startswith(\"GEN\")],\n",
    "    )\n",
    "    df = df.sort_values([\"axiom1_name\", \"axiom2_name\"])\n",
    "\n",
    "    tri = tril\n",
    "    if \"TREC\" in df[\"data_run_type\"].iloc[0]:\n",
    "        tri = triu\n",
    "\n",
    "    df = df.pivot(\n",
    "        columns=[\"axiom1_name\"],\n",
    "        index=[\"axiom2_name\"],\n",
    "        values=\"consistency\",\n",
    "    )\n",
    "    df = df.round(2)\n",
    "\n",
    "    data = df.values\n",
    "    mask = tri(data)\n",
    "\n",
    "    heatmap(data=df, mask=mask, **kwargs)\n",
    "\n",
    "\n",
    "df_plot = df_consistency_inter.copy()\n",
    "df_plot[\"data_run_type\"] = df_plot[\"data_run_type\"].map(data_run_type_display_names)\n",
    "\n",
    "plot = FacetGrid(\n",
    "    data=df_plot,\n",
    "    col=\"data_run_type\",\n",
    "    margin_titles=True,\n",
    "    height=5,\n",
    "    aspect=1,\n",
    "    # xlim=(0.2, 1),\n",
    "    # ylim=(0.72, 1),\n",
    ")\n",
    "plot.map_dataframe(\n",
    "    df_heatmap,\n",
    "    # vmin=0,\n",
    "    vmin=min_consistency,\n",
    "    vmax=1,\n",
    "    # cmap=\"flare_r\",\n",
    "    # cmap=\"crest_r\",\n",
    "    cmap=\"rocket\",\n",
    "    # cmap=cubehelix_palette(as_cmap=True, reverse=True),\n",
    "    linewidth=0.5,\n",
    "    linecolor='white',\n",
    ")\n",
    "plot.set_titles(\n",
    "    row_template=\"{|row_name}\",\n",
    "    col_template=\"{col_name}\",\n",
    ")\n",
    "plot.set_axis_labels(\"\", \"\")\n",
    "plot.savefig(\"../data/figures/figure-axioms-mutual-consistency-pre.pdf\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

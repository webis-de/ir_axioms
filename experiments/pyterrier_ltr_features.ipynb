{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aggregating Axiom Preferences for Learning to Rank\n",
    "\n",
    "The notebook below exemplifies how `ir_axioms` can be used to generate features for arbitrary learning-to-rank approaches like LambdMART in PyTerrier by aggregating pairwise preferences from different axioms.\n",
    "We use run files and qrels from the passage retrieval task of the TREC Deep Learning track in 2019 and 2020 as example (using BM25 as a baseline).\n",
    "In this notebook, we first generate learning-to-rank features by aggregating preferences from 2019 qrels and topics. Then we train a LambdaMART re-ranker with the generated axiomatic features, re-rank using the trained LambdaMART re-ranker, and evaluate nDCG@10, reciprocal rank, and average precision for the baseline and the re-ranked pipeline using PyTerrier's standard `Experiment` functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if \"google.colab\" in modules:\n",
    "    !pip install -q ir_axioms[experiments] python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We initialize PyTerrier and import all required libraries and load the data from [ir_datasets](https://ir-datasets.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier import started, init\n",
    "\n",
    "if not started():\n",
    "    init(tqdm=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets and Index\n",
    "\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")\n",
    "dataset_train: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2019/judged\")\n",
    "dataset_test: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2020/judged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Run\n",
    "\n",
    "We use PyTerrier's `BatchRetrieve` to create a baseline search pipeline for retrieving with BM25 from the index we just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.batchretrieve import BatchRetrieve\n",
    "\n",
    "bm25 = BatchRetrieve(str(index_dir.absolute()), wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Axioms\n",
    "\n",
    "Here we're listing which axioms we want to use in our experiments.\n",
    "Because some axioms require API calls or are computationally expensive, we cache all axioms using `ir_axiom`'s tilde operator (`~`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC,\n",
    "    QTArg,\n",
    "    QTPArg,\n",
    "    aSL,\n",
    "    PROX1,\n",
    "    PROX2,\n",
    "    PROX3,\n",
    "    PROX4,\n",
    "    PROX5,\n",
    "    TFC1,\n",
    "    TFC3,\n",
    "    RS_TF,\n",
    "    RS_TF_IDF,\n",
    "    RS_BM25,\n",
    "    RS_PL2,\n",
    "    RS_QL,\n",
    "    AND,\n",
    "    LEN_AND,\n",
    "    M_AND,\n",
    "    LEN_M_AND,\n",
    "    DIV,\n",
    "    LEN_DIV,\n",
    "    M_TDC,\n",
    "    LEN_M_TDC,\n",
    "    STMC1,\n",
    "    STMC1_f,\n",
    "    STMC2,\n",
    "    STMC2_f,\n",
    "    LNC1,\n",
    "    TF_LNC,\n",
    "    LB1,\n",
    "    REG,\n",
    "    ANTI_REG,\n",
    "    REG_f,\n",
    "    ANTI_REG_f,\n",
    "    ASPECT_REG,\n",
    "    ASPECT_REG_f,\n",
    "    ORIG,\n",
    ")\n",
    "\n",
    "axioms = [\n",
    "    ~ArgUC(),\n",
    "    ~QTArg(),\n",
    "    ~QTPArg(),\n",
    "    ~aSL(),\n",
    "    ~LNC1(),\n",
    "    ~TF_LNC(),\n",
    "    ~LB1(),\n",
    "    ~PROX1(),\n",
    "    ~PROX2(),\n",
    "    ~PROX3(),\n",
    "    ~PROX4(),\n",
    "    ~PROX5(),\n",
    "    ~REG(),\n",
    "    ~REG_f(),\n",
    "    ~ANTI_REG(),\n",
    "    ~ANTI_REG_f(),\n",
    "    ~ASPECT_REG(),\n",
    "    ~ASPECT_REG_f(),\n",
    "    ~AND(),\n",
    "    ~LEN_AND(),\n",
    "    ~M_AND(),\n",
    "    ~LEN_M_AND(),\n",
    "    ~DIV(),\n",
    "    ~LEN_DIV(),\n",
    "    ~RS_TF(),\n",
    "    ~RS_TF_IDF(),\n",
    "    ~RS_BM25(),\n",
    "    ~RS_PL2(),\n",
    "    ~RS_QL(),\n",
    "    ~TFC1(),\n",
    "    ~TFC3(),\n",
    "    ~M_TDC(),\n",
    "    ~LEN_M_TDC(),\n",
    "    ~STMC1(),\n",
    "    ~STMC1_f(),\n",
    "    ~STMC2(),\n",
    "    ~STMC2_f(),\n",
    "    ORIG(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To reduce the preference matrices to lists of three features, we define three aggregation functions.\n",
    "In this exemplary aggregations, we just count how often an axiom wants to change the original ordering in either direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aggregations = [\n",
    "    lambda prefs: sum(p >= 0 for p in prefs) / len(prefs),\n",
    "    lambda prefs: sum(p == 0 for p in prefs) / len(prefs),\n",
    "    lambda prefs: sum(p <= 0 for p in prefs) / len(prefs),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregating Axiomatic Features\n",
    "\n",
    "With the axioms and aggregation functions, we create axiomatic preference features for the top-20 results from the baseline ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ir_axioms.backend.pyterrier.transformers import AggregatedAxiomaticPreferences\n",
    "\n",
    "features = bm25 % 20 >> AggregatedAxiomaticPreferences(\n",
    "    axioms=axioms,\n",
    "    index=index_dir,\n",
    "    aggregations=aggregations,\n",
    "    dataset=dataset_name,\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This example shows how the features look like for the first topic of the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features.transform(dataset_train.get_topics()[:1])[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Learning to Rank\n",
    "\n",
    "After aggregating the preference features, we can use the features with any learning-to-rank approach.\n",
    "In this case, we initialize a LambdaMART ranker for optimizing nDCG@10 and apply it to our PyTerrier pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "from pyterrier.ltr import apply_learned_model\n",
    "\n",
    "lambda_mart = LGBMRanker(\n",
    "    num_iterations=1000,\n",
    "    metric=\"ndcg\",\n",
    "    eval_at=[10],\n",
    "    importance_type=\"gain\",\n",
    ")\n",
    "ltr = features >> apply_learned_model(lambda_mart, form=\"ltr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We train the LamdaMART ranker with the training dataset (using the last 5 topics as the validation dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ltr.fit(\n",
    "    dataset_train.get_topics()[:-5],\n",
    "    dataset_train.get_qrels(),\n",
    "    dataset_train.get_topics()[-5:],\n",
    "    dataset_train.get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimental Evaluation\n",
    "\n",
    "Because our axiomatic re-rankers are PyTerrier modules, we can now use PyTerrier's `Experiment` interface to evaluate various metrics and to compare our new approach to the BM25 baseline ranking.\n",
    "Refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io/en/latest/experiments.html) to learn more about running experiments.\n",
    "(We concatenate results from the Baseline ranking for the ranking positions after the top-20 using the `^` operator.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyterrier.pipelines import Experiment\n",
    "from ir_measures import nDCG, MAP, RR\n",
    "\n",
    "experiment = Experiment(\n",
    "    [bm25, ltr ^ bm25],\n",
    "    dataset_test.get_topics(),\n",
    "    dataset_test.get_qrels(),\n",
    "    [nDCG @ 10, RR, MAP],\n",
    "    [\"BM25\", \"Axiomatic LTR\"],\n",
    "    verbose=True,\n",
    ")\n",
    "experiment.sort_values(by=\"nDCG@10\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extra: Feature Importances\n",
    "\n",
    "Inspecting the feature importances from LambdaMART can help to identify axioms or aggregations that are not used for re-ranking.\n",
    "If an axiom's feature importance is zero for most of your applications, you may consider omitting it from the ranking pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "feature_importance: ndarray = lambda_mart.feature_importances_.reshape(\n",
    "    -1, len(aggregations)\n",
    ")\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance.sum(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

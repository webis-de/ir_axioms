{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan, isnan\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "from statistics import mean\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from numpy import array, stack, float_, sign\n",
    "from numpy.typing import NDArray\n",
    "from pandas import read_json, DataFrame, isna, concat, Categorical\n",
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "from pyterrier.terrier import IterDictIndexer, IndexFactory\n",
    "from seaborn import FacetGrid, histplot, lineplot, heatmap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from axioms.axiom import (\n",
    "    Axiom,\n",
    "    # Adapted axioms\n",
    "    GEN_aSL,\n",
    "    # GEN_ArgUC,\n",
    "    # GEN_QTArg,\n",
    "    # GEN_QTPArg,\n",
    "    GEN_LNC1,\n",
    "    GEN_TF_LNC,\n",
    "    GEN_PROX1,\n",
    "    GEN_PROX2,\n",
    "    GEN_PROX3,\n",
    "    GEN_PROX4,\n",
    "    GEN_PROX5,\n",
    "    GEN_REG,\n",
    "    # GEN_ANTI_REG,\n",
    "    GEN_AND,\n",
    "    # GEN_LEN_AND,\n",
    "    # GEN_M_AND,\n",
    "    GEN_DIV,\n",
    "    # GEN_LEN_DIV,\n",
    "    GEN_TFC1,\n",
    "    GEN_STMC1,\n",
    "    GEN_STMC2,\n",
    "    # Generation-specific axioms\n",
    "    CLAR1,\n",
    "    CLAR2,\n",
    "    CLAR3,\n",
    "    CLAR4,\n",
    "    CLAR5,\n",
    "    CLAR6,\n",
    "    CLAR7,\n",
    "    CONS1,\n",
    "    CONS2,\n",
    "    CONS3,\n",
    "    CONS4,\n",
    "    COV1,\n",
    "    COV2,\n",
    "    COV3,\n",
    "    COV4,\n",
    "    COV5,\n",
    "    # Oracle axioms\n",
    "    TrecRagNuggetAxiom,\n",
    "    TrecRagCrowdAxiom,\n",
    "    # Utility axioms\n",
    "    MajorityVoteAxiom,\n",
    ")\n",
    "from axioms.model import GenerationInput, GenerationOutput, Preference, PreferenceMatrix\n",
    "from axioms.tools import KeyBertAspectExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path(\"../data/cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_assignments_path = Path(\"../data/nugget_assignment.20241108.jl\")\n",
    "rag_crowd_responses_path = Path(\"../data/crowd/responses.jsonl.gz\")\n",
    "rag_crowd_ratings_path = Path(\"../data/crowd/ratings.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rag_assignments_outputs_df() -> DataFrame:\n",
    "    df = read_json(rag_assignments_path, lines=True)\n",
    "    df.drop(columns=[\"response_length\", \"nuggets\"], inplace=True)\n",
    "    df[\"query\"] = df[\"query\"].fillna(\"\")\n",
    "    df[\"answer_text\"] = df[\"answer_text\"].fillna(\"\")\n",
    "    df.rename(columns={\"run_id\": \"name\", \"answer_text\": \"text\"}, inplace=True)\n",
    "    df[\"context\"] = nan\n",
    "    return df[[\"qid\", \"query\", \"context\", \"name\", \"text\"]]\n",
    "\n",
    "read_rag_assignments_outputs_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rag_crowd_outputs_df() -> DataFrame:\n",
    "    df = read_json(rag_crowd_responses_path, lines=True)\n",
    "\n",
    "    df_ratings = read_json(rag_crowd_ratings_path, lines=True)\n",
    "    ratings_responses = concat([df_ratings[\"response_a\"], df_ratings[\"response_b\"]]).unique()\n",
    "    df = df[df[\"response\"].isin(ratings_responses)]\n",
    "\n",
    "    df[\"name\"] = df[\"kind\"] + \"_\" + df[\"style\"]\n",
    "    df = df[df[\"kind\"] == \"human\"]\n",
    "    df.drop(columns=[\"references_ids\", \"cleaned_text\", \"statements\", \"kind\", \"style\", \"response\"], inplace=True)\n",
    "    df.rename(columns={\"topic\": \"qid\", \"references_texts\": \"context\", \"raw_text\": \"text\"}, inplace=True)\n",
    "\n",
    "    return df[[\"qid\", \"query\", \"context\", \"name\", \"text\"]]\n",
    "\n",
    "read_rag_crowd_outputs_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inputs_outputs(df: DataFrame) -> list[tuple[GenerationInput, list[GenerationOutput]]]:\n",
    "    data: list[tuple[GenerationInput, list[GenerationOutput]]] = []\n",
    "    runs = sorted(df[\"name\"].unique())\n",
    "    for (query_id, query), df_query in df.groupby(\n",
    "        [\"qid\", \"query\"], group_keys=False, sort=False, as_index=False,\n",
    "    ):\n",
    "        contexts = {\n",
    "            tuple(row[\"context\"]) \n",
    "            if isinstance(row[\"context\"], Sequence) else None\n",
    "            for _, row in df_query.iterrows()\n",
    "        }\n",
    "        if len(contexts) > 1:\n",
    "            raise ValueError(f\"Multiple contexts for query {query_id}: {'; '.join(contexts)}\")\n",
    "        context = next(iter(contexts))\n",
    "        input = GenerationInput(\n",
    "            id=query_id,\n",
    "            text=query,\n",
    "            context=context,\n",
    "        )\n",
    "        df_query = df_query.drop(columns=[\"qid\", \"query\"])\n",
    "        outputs = {\n",
    "            row[\"name\"]: GenerationOutput(\n",
    "                id=row[\"name\"],\n",
    "                text=row[\"text\"],\n",
    "            )\n",
    "            for _, row in df_query.iterrows()\n",
    "        }\n",
    "        data.append(\n",
    "            (\n",
    "                input,\n",
    "                [\n",
    "                    outputs.get(\n",
    "                        run,\n",
    "                        GenerationOutput(\n",
    "                            id=run,\n",
    "                            text=\"\",\n",
    "                        ),\n",
    "                    )\n",
    "                    for run in runs\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_inputs_outputs(read_rag_assignments_outputs_df())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_inputs_outputs(read_rag_crowd_outputs_df())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_oracle_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput]]] = [\n",
    "    *[\n",
    "        (\n",
    "            f\"ORACLE-NUGGET-{score_type.upper()}{\"-STRICT\" if strict else ''}{f'-{margin_fraction:.1f}' if margin_fraction > 0.0 else ''}\",\n",
    "            TrecRagNuggetAxiom(\n",
    "                assignments_path=rag_assignments_path,\n",
    "                score_type=score_type,\n",
    "                strict=False,\n",
    "                margin_fraction=margin_fraction,\n",
    "            ),\n",
    "            \"assignments\",\n",
    "        )\n",
    "        for score_type in (\n",
    "            \"all\",\n",
    "            \"vital\",\n",
    "            \"weighted\"\n",
    "        )\n",
    "        for strict in (\n",
    "            True,\n",
    "            False,\n",
    "        )\n",
    "        for margin_fraction in (\n",
    "            # 0.0,\n",
    "            0.1,\n",
    "            # 0.2,\n",
    "            # 0.3,\n",
    "            # 0.4,\n",
    "            # 0.5,\n",
    "        )\n",
    "    ],\n",
    "    *[\n",
    "        (\n",
    "            f\"ORACLE-CROWD-{utility_type.upper()}{f'-{margin_fraction:.1f}' if margin_fraction > 0.0 else ''}\",\n",
    "            TrecRagCrowdAxiom(\n",
    "                responses_path=rag_crowd_responses_path,\n",
    "                ratings_path=rag_crowd_ratings_path,\n",
    "                utility_type=utility_type,\n",
    "                margin_fraction=margin_fraction,\n",
    "            ),\n",
    "            \"crowd\",\n",
    "        )\n",
    "        for utility_type in (\n",
    "            \"overall\",\n",
    "            \"coherence\",\n",
    "            \"consistency\",\n",
    "            \"correctness\",\n",
    "            \"coverage\",\n",
    "        )\n",
    "        for margin_fraction in (\n",
    "            # 0.0,\n",
    "            0.1,\n",
    "            # 0.2,\n",
    "            # 0.3,\n",
    "            # 0.4,\n",
    "            # 0.5,\n",
    "        )\n",
    "    ],\n",
    "]\n",
    "rag_oracle_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"), run_type)\n",
    "    for name, axiom, run_type in rag_oracle_axioms\n",
    "]\n",
    "rag_oracle_axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_oracle_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom, axiom_run_type in rag_oracle_axioms\n",
    "    if data_run_type == axiom_run_type\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted retrieval axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput]]] = [\n",
    "    (\"GEN-TFC1\", GEN_TFC1()),\n",
    "    (\"GEN-LNC1\", GEN_LNC1()),\n",
    "    (\"GEN-REG\", GEN_REG()),\n",
    "    (\"GEN-AND\", GEN_AND()),\n",
    "    (\"GEN-DIV\", GEN_DIV()),\n",
    "    (\"GEN-STMC1\", GEN_STMC1()),\n",
    "    (\"GEN-STMC2\", GEN_STMC2()),\n",
    "    (\"GEN-PROX1\", GEN_PROX1()),\n",
    "    (\"GEN-PROX2\", GEN_PROX2()),\n",
    "    (\"GEN-PROX3\", GEN_PROX3()),\n",
    "    (\"GEN-PROX4\", GEN_PROX4()),\n",
    "    (\"GEN-PROX5\", GEN_PROX5()),\n",
    "    (\"GEN-aSL\", GEN_aSL()),\n",
    "    (\"GEN-TF-LNC\", GEN_TF_LNC()),\n",
    "]\n",
    "rag_retrieval_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in rag_retrieval_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in rag_retrieval_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New generation axiom preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_bert = KeyBertAspectExtraction()\n",
    "rag_generation_axioms: list[tuple[str, Axiom[GenerationInput, GenerationOutput]]] = [\n",
    "    (\"CLAR1\", CLAR1()),\n",
    "    (\"CLAR2\", CLAR2()),\n",
    "    (\"CLAR3\", CLAR3()),\n",
    "    (\"CLAR4\", CLAR4()),\n",
    "    (\"CLAR5\", CLAR5()),\n",
    "    (\"CLAR6\", CLAR6()),\n",
    "    (\"CLAR7\", CLAR7()),\n",
    "    (\"CONS1\", CONS1()),\n",
    "    (\"CONS1-KB\", CONS1(aspect_extraction=key_bert)),\n",
    "    (\"CONS2\", CONS2()),\n",
    "    (\"CONS2-KB\", CONS2(aspect_extraction=key_bert)),\n",
    "    (\"CONS3\", CONS3()),\n",
    "    (\"CONS3-KB\", CONS3(aspect_extraction=key_bert)),\n",
    "    (\"CONS4\", CONS4()),\n",
    "    (\"COV1\", COV1()),\n",
    "    (\"COV1-KB\", COV1(aspect_extraction=key_bert)),\n",
    "    (\"COV2\", COV2()),\n",
    "    (\"COV2-KB\", COV2(aspect_extraction=key_bert)),\n",
    "    (\"COV3\", COV3()),\n",
    "    (\"COV3-KB\", COV3(aspect_extraction=key_bert)),\n",
    "    (\"COV4\", COV4()),\n",
    "    (\"COV5\", COV5()),\n",
    "]\n",
    "rag_generation_axioms = [\n",
    "    (name, axiom.cached(cache_path / \"axioms\" / f\"{name}.cache\"))\n",
    "    for name, axiom in rag_generation_axioms\n",
    "]\n",
    "# TODO: Add majority vote / consensus axiom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generation_axiom_prefs = [\n",
    "    (name, data_run_type, stack(\n",
    "        [\n",
    "            axiom.preferences(input, outputs)\n",
    "            for input, outputs in tqdm(data, desc=name, unit=\"query\")\n",
    "        ]\n",
    "    ))\n",
    "    for data, data_run_type in (\n",
    "        (to_inputs_outputs(read_rag_assignments_outputs_df()), \"assignments\"),\n",
    "        (to_inputs_outputs(read_rag_crowd_outputs_df()), \"crowd\"),\n",
    "    )\n",
    "    for name, axiom in rag_generation_axioms\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here we count the number matches for all types of preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for oracle_name, oracle_data_run_type, oracle_preferences in rag_oracle_prefs:\n",
    "    # Normalize preferences to -1, 0, 1.\n",
    "    oracle_preferences = sign(oracle_preferences)\n",
    "\n",
    "    for axiom_type, axiom_prefs in (\n",
    "        (\"adapted retrieval axiom\", rag_retrieval_axiom_prefs),\n",
    "        (\"generation-specific axiom\", rag_generation_axiom_prefs),\n",
    "    ):\n",
    "\n",
    "        for axiom_name, axiom_data_run_type, axiom_preferences in axiom_prefs:\n",
    "            # Normalize preferences to -1, 0, 1.\n",
    "            axiom_preferences = sign(axiom_preferences)\n",
    "\n",
    "            if oracle_data_run_type != axiom_data_run_type:\n",
    "                continue\n",
    "\n",
    "            for oracle_preference in (-1, 0, 1):\n",
    "                for axiom_preference in (-1, 0, 1):\n",
    "\n",
    "                    matching_preferences = (oracle_preferences == oracle_preference) & (\n",
    "                        axiom_preferences == axiom_preference\n",
    "                    )\n",
    "\n",
    "                    df_data.append(\n",
    "                        {\n",
    "                            \"data_run_type\": oracle_data_run_type,\n",
    "                            \"oracle_name\": oracle_name,\n",
    "                            \"axiom_name\": axiom_name,\n",
    "                            \"axiom_type\": axiom_type,\n",
    "                            \"oracle_preference\": oracle_preference,\n",
    "                            \"axiom_preference\": axiom_preference,\n",
    "                            \"count\": matching_preferences.sum(),\n",
    "                        }\n",
    "                    )\n",
    "df_distribution = DataFrame(df_data)\n",
    "df_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = (\n",
    "    df_distribution\n",
    "    .groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\"])[[\"count\"]]\n",
    "    .sum()\n",
    "    .rename(columns={\"count\": \"total_count\"})\n",
    ")\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axiom decisiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decisiveness = df_distribution.copy()\n",
    "df_decisiveness = df_decisiveness[df_decisiveness[\"axiom_preference\"] != 0]\n",
    "df_decisiveness = (\n",
    "    df_decisiveness.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\", \"axiom_type\"])[[\"count\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"count\": \"non_zero_count\"})\n",
    ")\n",
    "df_decisiveness = df_decisiveness.merge(total_counts, on=[\"data_run_type\", \"oracle_name\", \"axiom_name\"])\n",
    "\n",
    "df_decisiveness = df_decisiveness.groupby(by=list(set(df_decisiveness.columns) - {\"oracle_name\"})).first().reset_index().drop(columns=\"oracle_name\")\n",
    "\n",
    "df_decisiveness[\"zero_count\"] = (\n",
    "    df_decisiveness[\"total_count\"] - df_decisiveness[\"non_zero_count\"]\n",
    ")\n",
    "df_decisiveness[\"decisiveness\"] = (\n",
    "    df_decisiveness[\"non_zero_count\"] / df_decisiveness[\"total_count\"]\n",
    ")\n",
    "\n",
    "df_decisiveness = df_decisiveness[\n",
    "    [\"data_run_type\", \"axiom_name\", \"axiom_type\", \"zero_count\", \"non_zero_count\", \"decisiveness\"]\n",
    "]\n",
    "\n",
    "df_decisiveness.sort_values(\n",
    "    [\"data_run_type\", \"decisiveness\"],\n",
    "    ascending=[True, False],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axiom consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consistency = df_distribution.copy()\n",
    "df_consistency = df_consistency[df_consistency[\"axiom_preference\"] != 0]\n",
    "\n",
    "# Case 1: A preference pair of 0 and 1 is consistent (must not contradict).\n",
    "df_consistency = df_consistency[(df_consistency[\"axiom_preference\"] - df_consistency[\"oracle_preference\"]).abs() <= 1]\n",
    "# Case 2: A preference pair of 0 and 1 is inconsistent (must exactly match).\n",
    "# df_consistency = df_consistency[df_consistency[\"axiom_preference\"] == df_consistency[\"oracle_preference\"]]\n",
    "\n",
    "df_consistency = df_consistency.groupby([\"data_run_type\", \"oracle_name\", \"axiom_name\"])[[\"count\"]].sum().reset_index().rename(columns={\"count\": \"consistent_count\"})\n",
    "\n",
    "df_consistency = df_consistency.merge(df_decisiveness, on=[\"data_run_type\",\"axiom_name\"])\n",
    "df_consistency[\"consistency\"] = df_consistency[\"consistent_count\"] / df_consistency[\"non_zero_count\"]\n",
    "df_consistency[\"consistency\"] = df_consistency[\"consistency\"].fillna(1)\n",
    "\n",
    "df_consistency.sort_values(\n",
    "    [\"data_run_type\", \"oracle_name\", \"consistency\"],\n",
    "    ascending=[True, True, False],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined axiom effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_effectiveness = df_consistency.copy()\n",
    "\n",
    "# Harmonic mean of decisiveness and consistency\n",
    "df_effectiveness[\"effectiveness\"] = 2 * (df_effectiveness[\"decisiveness\"] * df_effectiveness[\"consistency\"]) / (df_effectiveness[\"decisiveness\"] + df_effectiveness[\"consistency\"])\n",
    "\n",
    "df_effectiveness.sort_values(\n",
    "    [\"data_run_type\", \"oracle_name\", \"effectiveness\"],\n",
    "    ascending=[True, True, False],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_effectiveness.groupby([\"data_run_type\", \"oracle_name\"])[[\"decisiveness\",\"consistency\", \"effectiveness\"]].mean().sort_values(by=[\"data_run_type\", \"effectiveness\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_effectiveness.groupby([\"data_run_type\", \"axiom_name\", \"axiom_type\"])[[\"decisiveness\",\"consistency\", \"effectiveness\"]].mean().sort_values(by=[\"data_run_type\", \"axiom_type\", \"effectiveness\"], ascending=[True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables for axiom effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_display_names = {\n",
    "    \"ORACLE-NUGGET-ALL-0.1\": r\"$A$\",\n",
    "    \"ORACLE-NUGGET-ALL-STRICT-0.1\": r\"$A_\\text{strict}$\",\n",
    "    \"ORACLE-NUGGET-VITAL-0.1\": r\"$V$\",\n",
    "    \"ORACLE-NUGGET-VITAL-STRICT-0.1\": r\"$V_\\text{strict}$\",\n",
    "    \"ORACLE-NUGGET-WEIGHTED-0.1\": r\"$W$\",\n",
    "    \"ORACLE-NUGGET-WEIGHTED-STRICT-0.1\": r\"$W_\\text{strict}$\",\n",
    "    \"ORACLE-CROWD-OVERALL-0.1\": r\"Overall\",\n",
    "    \"ORACLE-CROWD-COHERENCE-0.1\": r\"Coh.\",\n",
    "    \"ORACLE-CROWD-CONSISTENCY-0.1\": r\"Cons.\",\n",
    "    \"ORACLE-CROWD-CORRECTNESS-0.1\": r\"Corr.\",\n",
    "    \"ORACLE-CROWD-COVERAGE-0.1\": r\"Cov.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run_type_display_names = {\n",
    "    \"assignments\": r\"TREC 2024 RAG\",\n",
    "    \"crowd\": r\"Crowd-sourced\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "axiom_names = [\n",
    "    \"TFC1\",\n",
    "    \"TFC3\",\n",
    "    \"M-TDC\",\n",
    "    \"LNC1\",\n",
    "    \"TF-LNC\",\n",
    "    \"LB1\",\n",
    "    \"REG\",\n",
    "    \"AND\",\n",
    "    \"DIV\",\n",
    "    \"STMC1\",\n",
    "    \"STMC2\",\n",
    "    \"PROX1\",\n",
    "    \"PROX2\",\n",
    "    \"PROX3\",\n",
    "    \"PROX4\",\n",
    "    \"PROX5\",\n",
    "    \"ArgUC\",\n",
    "    \"QTArg\",\n",
    "    \"QTPArg\",\n",
    "    \"aSL\",\n",
    "    \"CLAR1\",\n",
    "    \"CLAR2\",\n",
    "    \"CLAR3\",\n",
    "    \"CLAR4\",\n",
    "    \"CLAR5\",\n",
    "    \"CLAR6\",\n",
    "    \"CLAR7\",\n",
    "    \"CONS1\",\n",
    "    \"CONS2\",\n",
    "    \"CONS3\",\n",
    "    \"CONS4\",\n",
    "    \"COV1\",\n",
    "    \"COV2\",\n",
    "    \"COV3\",\n",
    "    \"COV4\",\n",
    "    \"COV5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "axiom_name_renaming = {\n",
    "    \"CONS1\": nan,\n",
    "    \"CONS1-KB\": \"CONS1\",\n",
    "    \"CONS2\": nan,\n",
    "    \"CONS2-KB\": \"CONS2\",\n",
    "    \"CONS3\": nan,\n",
    "    \"CONS3-KB\": \"CONS3\",\n",
    "    \"COV1\": nan,\n",
    "    \"COV1-KB\": \"COV1\",\n",
    "    \"COV2\": nan,\n",
    "    \"COV2-KB\": \"COV2\",\n",
    "    \"COV3\": nan,\n",
    "    \"COV3-KB\": \"COV3\",\n",
    "    \"COV4\": nan,\n",
    "    \"COV4-KB\": \"COV4\",\n",
    "    \"COV5\": nan,\n",
    "    \"COV5-KB\": \"COV5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_run_type = \"assignments\"\n",
    "df_table = df_effectiveness.copy()\n",
    "\n",
    "\n",
    "df_table[\"axiom_name\"] = Categorical(df_table['axiom_name'].str.removeprefix(\"GEN-\").replace(axiom_name_renaming), axiom_names)\n",
    "\n",
    "df_table[\"oracle_name\"] = Categorical(df_table[\"oracle_name\"], list(oracle_display_names.keys()))\n",
    "df_table[\"data_run_type\"] = Categorical(df_table[\"data_run_type\"], list(data_run_type_display_names.keys()))\n",
    "\n",
    "columns = [\"l\"]\n",
    "for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    columns += [\"r\"]\n",
    "    for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        columns += [\"r\"]\n",
    "print(r\"\\begin{tabular}{@{}\" + \"\".join(columns) + r\"@{}}\")\n",
    "print(r\"  \\toprule\")\n",
    "\n",
    "columns = [r\"\\textbf{Axiom}\"]\n",
    "for data_run_type, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    num_columns = 1\n",
    "    for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        num_columns += 1\n",
    "    columns += [r\"\\multicolumn{\" + f\"{num_columns}\" + r\"}{c}{\\textbf{\" + data_run_type_display_names[data_run_type] + r\"}}\"]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "columns = [\"\"]\n",
    "i = 2\n",
    "for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    start = i\n",
    "    i += 1\n",
    "    for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        i += 1\n",
    "    columns += [r\"\\cmidrule(lr){\" + f\"{start}\" + r\"-\" + f\"{i-1}\" + r\"}\"]\n",
    "print(r\"  \" + r\"\".join(columns))\n",
    "\n",
    "columns = [\"\"]\n",
    "for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    columns += [r\"\\textbf{Dec.}\"]\n",
    "    num_columns = 0\n",
    "    for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        num_columns += 1\n",
    "    columns += [r\"\\multicolumn{\" + f\"{num_columns}\" + r\"}{c}{\\textbf{Consistency}}\"]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "columns = [\"\"]\n",
    "i = 2\n",
    "for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    columns += [r\"\\cmidrule(lr){\" + f\"{i}\" + r\"-\" + f\"{i}\" + r\"}\"]\n",
    "    i += 1\n",
    "    start = i\n",
    "    for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        i += 1\n",
    "    columns += [r\"\\cmidrule(lr){\" + f\"{start}\" + r\"-\" + f\"{i-1}\" + r\"}\"]\n",
    "print(r\"  \" + r\"\".join(columns))\n",
    "\n",
    "columns = [\"\"]\n",
    "for _, df_data_run_type in df_table.groupby(\"data_run_type\"):\n",
    "    if len(df_data_run_type) == 0:\n",
    "        continue\n",
    "    columns += [\"\"]\n",
    "    for oracle_name, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "        if len(df_oracle) == 0:\n",
    "            continue\n",
    "        columns += [r\"\\multicolumn{1}{c}{\" + oracle_display_names[oracle_name] + r\"}\"]\n",
    "print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "for axiom_type, df_axiom_type in df_table.groupby(\"axiom_type\"):\n",
    "    print(r\"  \\midrule\")\n",
    "    for axiom_name, df_axiom in df_axiom_type.groupby(\"axiom_name\"):\n",
    "        if len(df_axiom) <= 0:\n",
    "            continue\n",
    "        columns = [axiom_name.removeprefix(\"GEN-\")]\n",
    "        for data_run_type, df_data_run_type in df_axiom.groupby(\"data_run_type\"):\n",
    "            if len(df_data_run_type) == 0:\n",
    "                continue\n",
    "            if len(df_data_run_type[\"decisiveness\"].unique()) != 1:\n",
    "                raise ValueError()\n",
    "            decisiveness = df_data_run_type.iloc[0][\"decisiveness\"]\n",
    "            # decisiveness_prefix = \"\" if decisiveness > 0 else r\"\\color{gray} \"\n",
    "            # decisiveness_prefix = \"\"\n",
    "            decisiveness_prefix = r\"\\color{black!\" + f\"{(pow(decisiveness, 1/4)*100):.0f}\" + r\"!gray} \"\n",
    "            columns += [\n",
    "                decisiveness_prefix + f\"{decisiveness:0.0%}\".replace(\"%\", r\"\\%\"),\n",
    "            ]\n",
    "            for _, df_oracle in df_data_run_type.groupby(\"oracle_name\"):\n",
    "                if len(df_oracle) == 0:\n",
    "                    continue\n",
    "                if len(df_oracle) != 1:\n",
    "                    raise ValueError()\n",
    "                row = df_oracle.iloc[0]\n",
    "                columns += [\n",
    "                    decisiveness_prefix + f\"{row[\"consistency\"]:0.0%}\".replace(\"%\", r\"\\%\"),\n",
    "                ]\n",
    "        print(r\"  \" + r\" & \".join(columns).strip() + r\" \\\\\")\n",
    "print(r\"  \\bottomrule\")\n",
    "print(r\"\\end{tabular}\")\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(\n",
    "    data: DataFrame,\n",
    "    x: str,\n",
    "    y: str,\n",
    "    count=\"count\",\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    data = data.pivot(index=y, columns=x, values=count)\n",
    "    heatmap(data=data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_distribution.copy()\n",
    "# df_plot = df_plot[df_plot[\"oracle_name\"] == \"TREC-ORACLE-ALL\"]\n",
    "# df_plot = df_plot[df_plot[\"oracle_name\"] == \"TREC-ORACLE-VITAL\"]\n",
    "# df_plot = df_plot[df_plot[\"oracle_preference\"] != 0]\n",
    "# df_plot = df_plot[df_plot[\"axiom_preference\"] != 0]\n",
    "# df_plot = df_plot[df_plot[\"axiom_preference\"] != df_plot[\"oracle_preference\"]]\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = FacetGrid(\n",
    "#     data=df_plot,\n",
    "#     # col=\"oracle_threshold\",\n",
    "#     col=\"oracle_name\",\n",
    "#     row=\"axiom_name\",\n",
    "#     margin_titles=True,\n",
    "# )\n",
    "# plot.map_dataframe(\n",
    "#     draw_heatmap,\n",
    "#     x=\"oracle_preference\",\n",
    "#     y=\"axiom_preference\",\n",
    "#     count=\"count\",\n",
    "#     # vmin=0,\n",
    "#     # vmax=1,\n",
    "#     # norm=LogNorm(\n",
    "#     #     vmin=0,\n",
    "#     #     vmax=1,\n",
    "#     # ),\n",
    "#     norm=LogNorm(\n",
    "#         vmin=0.1,\n",
    "#         vmax=45 * 45 * 21,\n",
    "#         clip=True,\n",
    "#     ),\n",
    "#     square=True,\n",
    "#     cmap=\"rocket\",\n",
    "# )\n",
    "# plot.set_titles(\n",
    "#     row_template=\"{row_name}\",\n",
    "#     col_template=\"{col_name}\",\n",
    "# )\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot = df_consistency.copy()\n",
    "# df_plot = df_plot[\n",
    "#     ~df_plot[\"non_zero\"]\n",
    "#     & ~df_plot[\"strict\"]\n",
    "#     & (df_plot[\"oracle_name\"] == \"TREC-ORACLE-ALL\")\n",
    "#     # & (df_plot[\"oracle_name\"] == \"TREC-ORACLE-VITAL\")\n",
    "# ]\n",
    "# df_plot[\"decisiveness\"] = df_plot[\"axiom_proportion_non_zero\"] * df_plot[\"mean_consistency\"]\n",
    "# # df_plot[\"decisiveness\"] = df_plot[\"mean_consistency\"]\n",
    "# df_plot = df_plot.pivot(\n",
    "#     index=\"axiom_name\",\n",
    "#     columns=\"oracle_threshold\",\n",
    "#     values=\"decisiveness\",\n",
    "# )\n",
    "# df_plot = df_plot.fillna(0)\n",
    "# plot = heatmap(\n",
    "#     data=df_plot,\n",
    "#     cmap=\"rocket\",\n",
    "#     vmin=0,\n",
    "#     vmax=1,\n",
    "# )\n",
    "# # plot.set_titles(template=\"{col_name}\")\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot = df_consistency.copy()\n",
    "# df_plot = df_plot[\n",
    "#     df_plot[\"non_zero\"]\n",
    "#     & ~df_plot[\"strict\"]\n",
    "#     & (df_plot[\"oracle_name\"] == \"TREC-ORACLE-ALL\")\n",
    "# ]\n",
    "# df_plot = df_plot.pivot(\n",
    "#     index=\"axiom_name\",\n",
    "#     columns=\"oracle_threshold\",\n",
    "#     values=\"axiom_proportion_non_zero\",\n",
    "# )\n",
    "# df_plot = 1 - df_plot\n",
    "# plot = heatmap(\n",
    "#     data=df_plot,\n",
    "#     cmap=\"rocket\",\n",
    "#     vmin=0,\n",
    "#     vmax=1,\n",
    "# )\n",
    "# # plot.set_titles(template=\"{col_name}\")\n",
    "# plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

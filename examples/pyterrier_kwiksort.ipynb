{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KwikSort Re-Ranking\n",
    "\n",
    "The notebook below exemplifies how `ir_axioms` can be used to re-rank a result set in PyTerrier using the KwikSort algorithm and manually combined axioms.\n",
    "We use run files and qrels from the passage retrieval task of the TREC Deep Learning track in 2019 and 2020 as example (using BM25 as a baseline).\n",
    "In this notebook, we evaluate nDCG@10, reciprocal rank, and average precision for the baseline and the KwikSort-re-ranked pipeline using PyTerrier's standard `Experiment` functionality."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation\n",
    "\n",
    "Install the `ir_axioms` framework and [PyTerrier](https://github.com/terrier-org/pyterrier). In Google Colab, we do this automatically."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "\n",
    "if 'google.colab' in modules:\n",
    "    !pip install -q ir_axioms[examples] python-terrier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We initialize PyTerrier and import all required libraries and load the data from [ir_datasets](https://ir-datasets.com/)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from pyterrier import started, init\n",
    "\n",
    "if not started():\n",
    "    init(tqdm=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets and Index\n",
    "Using PyTerrier's `get_dataset()`, we load the MS MARCO passage ranking dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from pyterrier.datasets import get_dataset, Dataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset_name = \"msmarco-passage\"\n",
    "dataset: Dataset = get_dataset(f\"irds:{dataset_name}\")\n",
    "dataset_test: Dataset = get_dataset(f\"irds:{dataset_name}/trec-dl-2020/judged\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now define paths where we will store temporary files, datasets, and the search index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path(\"cache/\")\n",
    "index_dir = cache_dir / \"indices\" / dataset_name.split(\"/\")[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the index is not ready yet, now is a good time to create it and index the MS MARCO passages.\n",
    "(Lean back and relax as this may take a while...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "if not index_dir.exists():\n",
    "    indexer = IterDictIndexer(str(index_dir.absolute()))\n",
    "    indexer.index(\n",
    "        dataset.get_corpus_iter(),\n",
    "        fields=[\"text\"]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Run\n",
    "\n",
    "We use PyTerrier's `BatchRetrieve` to create a baseline search pipeline for retrieving with BM25 from the index we just created."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from pyterrier.batchretrieve import BatchRetrieve\n",
    "\n",
    "bm25 = BatchRetrieve(str(index_dir.absolute()), wmodel=\"BM25\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine Axioms\n",
    "Here we're combining many of the axioms implemented in `ir_axioms` to form a majority voting.\n",
    "That is, we only want to keep preferences, where at least 50% (or 0.5) of the axioms agree.\n",
    "Because some axioms require API calls or are computationally expensive, we cache the voting result using the tilde operator (`~`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from ir_axioms.axiom import (\n",
    "    ArgUC, QTArg, QTPArg, aSL, PROX1, PROX2, PROX3, PROX4, PROX5, TFC1, TFC3, RS_TF, RS_TF_IDF, RS_BM25, RS_PL2, RS_QL,\n",
    "    AND, LEN_AND, M_AND, LEN_M_AND, DIV, LEN_DIV, M_TDC, LEN_M_TDC, STMC1, STMC1_f, STMC2, STMC2_f, LNC1, TF_LNC, LB1,\n",
    "    REG, ANTI_REG, REG_f, ANTI_REG_f, ASPECT_REG, ASPECT_REG_f, ORIG, VoteAxiom\n",
    ")\n",
    "\n",
    "axiom = (\n",
    "        ~VoteAxiom([\n",
    "            ArgUC(), QTArg(), QTPArg(), aSL(),\n",
    "            LNC1(), TF_LNC(), LB1(),\n",
    "            PROX1(), PROX2(), PROX3(), PROX4(), PROX5(),\n",
    "            REG(), REG_f(), ANTI_REG(), ANTI_REG_f(), ASPECT_REG(), ASPECT_REG_f(),\n",
    "            AND(), LEN_AND(), M_AND(), LEN_M_AND(), DIV(), LEN_DIV(),\n",
    "            RS_TF(), RS_TF_IDF(), RS_BM25(), RS_PL2(), RS_QL(),\n",
    "            TFC1(), TFC3(), M_TDC(), LEN_M_TDC(),\n",
    "            STMC1(), STMC1_f(), STMC2(), STMC2_f(),\n",
    "        ], minimum_votes=0.5) | ORIG()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KwikSort Re-ranking\n",
    "After having defined the axiom to use for re-ranking, we create a new PyTerrier pipeline that re-ranks the top-20 baseline results using the KwikSort algorithm.\n",
    "KwikSort works similar to Quicksort, but instead of comparing items by natural order, it compares by axiom preferences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from ir_axioms.modules.pivot import MiddlePivotSelection\n",
    "from ir_axioms.backend.pyterrier.transformers import KwikSortReranker\n",
    "\n",
    "kwiksort = bm25 % 20 >> KwikSortReranker(\n",
    "    axiom=axiom,\n",
    "    index=index_dir,\n",
    "    dataset=dataset_name,\n",
    "    pivot_selection=MiddlePivotSelection(),\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimental Evaluation\n",
    "Because our axiomatic re-rankers are PyTerrier modules, we can now use PyTerrier's `Experiment` interface to evaluate various metrics and to compare our new approach to the BM25 baseline ranking.\n",
    "Refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io/en/latest/experiments.html) to learn more about running experiments.\n",
    "(We concatenate results from the Baseline ranking for the ranking positions after the top-20 using the `^` operator.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "pt.Experiment:   0%|          | 0/2 [00:00<?, ?system/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e228f541bb60400d802b84dcaee32cab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reranking query axiomatically:   0%|          | 0/54 [00:00<?, ?query/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e25a9a78511d49fa802182d621e5146c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyterrier.pipelines import Experiment\n",
    "from ir_measures import nDCG, MAP, RR\n",
    "\n",
    "experiment = Experiment(\n",
    "    [bm25, kwiksort ^ bm25],\n",
    "    dataset_test.get_topics(),\n",
    "    dataset_test.get_qrels(),\n",
    "    [nDCG @ 10, RR, MAP],\n",
    "    [\"BM25\", \"KwikSort\"],\n",
    "    verbose=True,\n",
    ")\n",
    "experiment.sort_values(by=\"nDCG@10\", ascending=False, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "       name   nDCG@10        RR        AP\n0      BM25  0.493627  0.802359  0.358724\n1  KwikSort  0.491858  0.802102  0.358587",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>nDCG@10</th>\n      <th>RR</th>\n      <th>AP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BM25</td>\n      <td>0.493627</td>\n      <td>0.802359</td>\n      <td>0.358724</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KwikSort</td>\n      <td>0.491858</td>\n      <td>0.802102</td>\n      <td>0.358587</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}